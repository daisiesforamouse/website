<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Mathematical Computation I: Matrix Computation Course</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <link rel="stylesheet" href="wiki.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Mathematical Computation I: Matrix Computation
Course</h1>
<h2 class="subtitle">UChicago STAT 30900, Autumn 2023</h2>
</header>
<hr>
<nav id="TOC" role="doc-toc">
<ul class="incremental">
<li><a href="#linear-algebra-review"
id="toc-linear-algebra-review">Linear Algebra Review</a></li>
<li><a href="#eigenvalues-and-eigenvectors"
id="toc-eigenvalues-and-eigenvectors">Eigenvalues and
Eigenvectors</a></li>
<li><a href="#singular-value-decomposition"
id="toc-singular-value-decomposition">Singular Value
Decomposition</a></li>
<li><a href="#applications-of-the-svd"
id="toc-applications-of-the-svd">Applications of the SVD</a></li>
<li><a href="#floating-point-arithmetic"
id="toc-floating-point-arithmetic">Floating Point Arithmetic</a></li>
<li><a href="#conditioning" id="toc-conditioning">Conditioning</a></li>
<li><a href="#stability" id="toc-stability">Stability</a></li>
<li><a href="#qr-decomposition" id="toc-qr-decomposition">QR
Decomposition</a></li>
</ul>
</nav>
<hr>
<h3 id="linear-algebra-review">Linear Algebra Review</h3>
<hr />
<p>By convention, vectors are column vectors.</p>
<p>Set <span class="math inline">V</span> a vector space (almost always
real or complex).</p>
<p><em>Def</em>: <span class="math inline">\| \cdot \| : V \to \mathbb
R</span> is a <strong>norm</strong> if it satisfies the following
properties:</p>
<ul class="incremental">
<li><span class="math inline">\|v\| \geq 0</span> for any <span
class="math inline">v \in V</span>,</li>
<li><span class="math inline">\|v\| = 0</span> iff <span
class="math inline">v = 0</span>,</li>
<li><span class="math inline">\|\alpha v\| = |\alpha| \|v\|</span> for
<span class="math inline">\alpha \in \mathbb R</span> and <span
class="math inline">v \in V</span>,</li>
<li><span class="math inline">\|v + w\| \leq \|v\| + \|w\|</span> for
any <span class="math inline">v, w \in V</span>.</li>
</ul>
<p>Now, since this is a computational class, we only care about specific
norms, almost all of which we can quickly write down.</p>
<h4 id="vector-norms">Vector Norms</h4>
<p>Set <span class="math inline">V = \mathbb R^n</span> or <span
class="math inline">\mathbb C^n</span> equivalently.</p>
<p><em>Def</em>: The <strong>Minkowski</strong> or <span
class="math inline">p</span><strong>-norm</strong> is given by <span
class="math display">
  \| x \|_p = \left(\sum_{i=1} x_i^p\right)^{1/p}
</span> and we call the <span class="math inline">2</span>-norm the
<strong>Euclidean norm</strong> and the <span
class="math inline">1</span>-norm the <strong>Manhattan
norm</strong>.</p>
<p><em>Def</em>: The <span
class="math inline">\infty</span><strong>-norm</strong> is the limit of
<span class="math inline">p</span>-norms as <span class="math inline">p
\to \infty</span>, and is given by <span class="math display">
  \| x \|_\infty = \max_{i = 1, \dots, n} |x_i| = \lim_{p \to \infty} \|
x \|_p.
</span></p>
<p><em>Def</em>: For a weight vector <span
class="math inline">\underline w = \begin{bmatrix}w_1, \dots,
w_n\end{bmatrix}^T \in \mathbb R^n</span>, with each <span
class="math inline">w_i &gt; 0</span>, we have that the
<strong>weighted</strong> <span
class="math inline">p</span><strong>-norm</strong> is <span
class="math display">
  \| v \|_{\underline w, p} = \left(\sum_{i=1} w_i x_i^p\right)^{1/p}.
</span></p>
<p><em>Def</em>: In general, for any positive definite matrix <span
class="math inline">A</span> (that is, <span class="math inline">x^TAx
&gt; 0</span> for all <span class="math inline">x \neq 0</span>), we may
consider the <strong>Mahalanobis norm</strong> <span
class="math display">
  \| v \|_{A} = \left(x^T A x\right)^{1/2}.
</span></p>
<p>As convention, the “default” norm when a subscript is omitted is the
Euclidean norm.</p>
<h4 id="matrix-norms">Matrix Norms</h4>
<p>Set <span class="math inline">V = \mathbb R^{m \times n}</span> or
<span class="math inline">\mathbb C^{m \times n}</span>
equivalently.</p>
<p><em>Def</em>: The <strong>Hölder</strong> <span
class="math inline">p</span><strong>-norms</strong> are given by <span
class="math display">
  \| X \|_{H, p} = \left(\sum_{i=1}^m\sum_{j=1}^m |x_{ij}|^p
\right)^{1/p},
</span> and the Hölder <span class="math inline">2</span>-norm is called
the Frobenius norm, which is also defined on infinite dimensional vector
spaces as <span class="math display">
  \| X \|_F = \left(\mathop{\mathrm{Tr}}(XX^*)\right)^{1/2}
</span> where <span class="math inline">^*</span> is the conjugate
transpose.</p>
<p><em>Def</em>: As before, we can take <span class="math inline">p \to
\infty</span> to get the <strong>Hölder</strong> <span
class="math inline">\infty</span><strong>-norm</strong> given by <span
class="math display">
  \| X\|_{H, \infty} = \max_{\substack{i = 1, \dots n \\ j = 1, \dots,
n}} |x_{ij}|.
</span></p>
<p><em>Def</em>: We can also define norms on matrices by viewing them as
linear maps <span class="math inline">A: \mathbb R^n \to \mathbb
R^m</span>; in particular, if we have some norm <span
class="math inline">\| \cdot \|_a</span> on <span
class="math inline">\mathbb R^n</span> and some norm <span
class="math inline">\| \cdot \|_b</span> on <span
class="math inline">\mathbb R^m</span>, we may define the
<strong>operator norm</strong> (or <strong>induced norm</strong>) <span
class="math display">
  \| A \|_{a, b} = \max_{x \neq 0} \frac{\| A x \|_b}{\| x \|_a}.
</span> In particular, if the norms on the domain and codomain are just
<span class="math inline">p</span>-norms, we write <span
class="math display">
  \| A \|_{p} = \max_{x \neq 0}\frac{\| A x\|_p}{\| x \|_p}
</span> and call it the <span
class="math inline">p</span><strong>-norm</strong> of <span
class="math inline">A</span>. In particular, we call the <span
class="math inline">2</span>-norm the spectral norm. Further, the <span
class="math inline">1</span>-norm and <span
class="math inline">\infty</span>-norm are just <span
class="math display">
  \| A \|_1 = \max_{j = 1, \dots, n} \left(\sum_{i=1}^m |a_{ij}|
\right),
</span> which is the max column sum, and <span class="math display">
  \| A \|_\infty = \max_{i = 1, \dots, m} \left(\sum_{j=1}^n |a_{ij}|
\right),
</span> which is the max row sum; both facts are easy to check.</p>
<p>In general, for <span class="math inline">p \notin \{1, 2,
\infty\}</span>, computing <span class="math inline">\| A \|_p</span> is
NP-hard, and if we consider <span class="math inline">\| A
\|_{p,q}</span> then <span class="math inline">\|A\|_{\infty, 1}</span>
is hard and <span class="math inline">\|A\|_{1, \infty}</span> is
easy.</p>
<h4 id="properties-of-norms">Properties of Norms</h4>
<p>We may also want to consider some other desirable properties on our
norms.</p>
<ul class="incremental">
<li>For example, we might want <strong>submultiplicativity</strong>:
<span class="math display">
\| A B\| \leq \|A \| \| B \|.
</span> The Frobenius norm is submultiplicative.</li>
<li>Take also <strong>consistency</strong>: <span class="math display">
\| Ax \|_b \leq \| A \|_{a,b} \|x\|_a.
</span> This is true for <span class="math inline">p</span>-norms, but
not in general.</li>
</ul>
<p>Some properties always hold.</p>
<p><strong>Prop</strong>: Every norm is Lipschitz.</p>
<p><em>Proof</em>: Let our norm be <span class="math inline">\| \cdot \|
: V \to \mathbb{R}</span>. The triangle inequality immediately implies
<span class="math display">
  | \| u \| - \| v \| | \leq \| u - v \|.
</span></p>
<p><strong>Theorem (Equivalence of Norms)</strong>: <a
href="https://kconrad.math.uconn.edu/blurbs/gradnumthy/equivnorms.pdf">Link</a>
Set <span class="math inline">V</span> a finite-dimensional vector
space. Then every pair of norms <span class="math inline">\| \cdot \|_a,
\| \cdot \|_b</span> are equivalent to each other, e.g. there are
constants <span class="math inline">c_1, c_2</span> such that for any
<span class="math inline">v \in V</span>, <span class="math display">
  c_1\| v \|_b \leq \| v \|_a \leq c_2 \| v \|_b.
</span></p>
<p><em>Proof</em>: Induct on the dimension of <span
class="math inline">V</span> and see that every norm is equivalent to
the infinity norm.</p>
<p><em>Def</em>: We say that a sequence <span class="math inline">\{ x_k
\}_{k=1}^\infty</span> of vectors <strong>converges</strong> to <span
class="math inline">x</span> if <span class="math display">
  \lim_{k \to \infty} \| x_k - x \| = 0.
</span></p>
<p>Then, the above clearly shows that convergence in one norm implies
convergence in every norm.</p>
<h4 id="inner-outer-matrix-products">Inner, Outer, Matrix Products</h4>
<p><em>Def</em>: Set <span class="math inline">V</span> a <span
class="math inline">K</span>-vector space (where <span
class="math inline">K = \mathbb{R}, \mathbb{C}</span>). An <strong>inner
product</strong> is a binary operation <span class="math inline">\langle
\cdot, \cdot \rangle: V \times V \to \mathbb{R}</span> which satisfies
that</p>
<ul class="incremental">
<li><span class="math inline">\left\langle v, v \right\rangle \geq
0</span> for all <span class="math inline">v \in V</span>,</li>
<li><span class="math inline">\left\langle v, v,\right\rangle = 0</span>
if and only if <span class="math inline">v = 0</span>,</li>
<li><span class="math inline">\left\langle u, v \right\rangle =
\overline{ \left\langle v, u\right\rangle }</span> for all <span
class="math inline">u, v \in V</span>,</li>
<li><span class="math inline">\left\langle \alpha_1 u_1 + \alpha_2 u_2,
v\right\rangle = \alpha \left\langle u_1, v \right\rangle + \alpha_2
\left\langle u_2, v\right\rangle</span> for all <span
class="math inline">u_1, u_2, v \in V, \alpha_1, \alpha_2 \in
K</span>.</li>
</ul>
<p><strong>Prop</strong>: For an inner product <span
class="math inline">\left\langle \cdot, \cdot \right\rangle</span>,
<span class="math display">
  \| v \| = \sqrt{ \left\langle v, v \right\rangle }
</span> is a norm. Furthermore, an arbitrary norm <span
class="math inline">\| \|</span> is induced by an inner product if and
only if it satisfies the parallelogram law <span class="math display">
  \|u + v\|^2 + \|u - v\|^2 = 2 \|u\| ^ 2 + 2 \|v\| ^2.
</span></p>
<p><strong>Theorem (Cauchy-Schwarz)</strong>: <a
href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Link</a>
Let <span class="math inline">\|\cdot\|</span> be induced by <span
class="math inline">\left\langle \cdot, \cdot \right\rangle</span>. Then
<span class="math display">
  \sqrt{\left\langle u, v \right\rangle} \leq \|u\| \|v\|.
</span></p>
<p><em>Def</em>: The <strong>standard Euclidean inner product</strong>
on <span class="math inline">\mathbb{C}^n</span> is <span
class="math display">
  \left\langle x, y\right\rangle = x^*y.
</span></p>
<p><em>Def</em>: The <strong>Frobenius</strong> norm on <span
class="math inline">\mathbb{C}^{m \times n}</span> is <span
class="math display">
  \left\langle X, Y\right\rangle = \sum_{i=1}^m \sum_{j=1}^n x_{ij}
y_{ij} = \mathop{\mathrm{Tr}}(X^*Y).
</span></p>
<p><strong>Theorem (Hölder Inequality)</strong>: <a
href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Link</a>
For <span class="math inline">x, y \in \mathbb{C}^n</span> and <span
class="math inline">p^{-1} + q^{-1} = 1</span>, we have <span
class="math display">
  |x^*y| \leq \|x\|_p \|y\|_q.
</span></p>
<p><strong>Theorem (Bessel Inequality)</strong>: <a
href="https://en.wikipedia.org/wiki/Bessel%27s_inequality">Link</a> For
<span class="math inline">x \in \mathbb{C}^n</span> and an orthonormal
basis <span class="math inline">e_1, \dots, e_n</span>, we have <span
class="math display">
  \sum_{k=1}^n \left| \left\langle x, e_k \right\rangle^2\right|  \leq
\|x\|_2.
</span></p>
<p><em>Def</em>: The <strong>outer product</strong> is a binary operator
<span class="math inline">\mathbb{C}^m \times \mathbb{C}^n \to
\mathbb{C}^{m \times n}</span> taking <span class="math display">
  (x, y) \mapsto xy^*.
</span></p>
<p><strong>Prop</strong>: <span class="math inline">A \in \mathbb{R}^{m
\times n}</span> is an outer product iff and only if it has rank 1.</p>
<p><em>Def</em>: The matrix product is a binary operator <span
class="math inline">\mathbb{C}^{m \times n} \times \mathbb{C}^{n \times
p} \to \mathbb{C}^{M \times p}</span>. Set <span class="math inline">A =
\left[\begin{matrix}\alpha_1 &amp; \alpha_2 &amp; \cdots &amp;
\alpha_m\end{matrix}\right]^T</span> and <span class="math inline">B =
\left[\begin{matrix}\beta_1 &amp; \beta_2 &amp; \cdots &amp;
\beta_p\end{matrix}\right]</span>. Then, <span class="math display">
  AB = \left[\begin{matrix}
    \alpha_1^T \beta_1 &amp; \cdots &amp; \alpha_1^T \beta_n \\
    \alpha_2^T \beta_1 &amp; \cdots &amp; \alpha_2^T \beta_n \\
    \vdots &amp; \ddots &amp; \vdots \\
    \alpha_m^T \beta_1 &amp; \cdots &amp; \alpha_m^T \beta_n
  \end{matrix}\right]
  = \left[\begin{matrix}A\beta_1 &amp; A\beta_2 &amp; \cdots &amp;
A\beta_n\end{matrix}\right]
  = \left[\begin{matrix}\alpha_1^TB \\ \alpha_2^TB \\ \vdots \\
\alpha_m^TB\end{matrix}\right].
</span> Alternatively, it is uniquely characterized as the matrix
representing the composition of <span class="math inline">A</span> and
<span class="math inline">B</span> as operators.</p>
<p><strong>Prop</strong>: Let <span class="math inline">D =
\mathop{\mathrm{diag}}(d_1, \dots, d_n)</span>, the diagonal matrix with
entries <span class="math inline">d_1, \dots, d_n</span>; then <span
class="math display">
  AD = \left[\begin{matrix}d_1\alpha_1, \dots, d_n
\alpha_n\end{matrix}\right].
</span> Simiar for the other direction of multiplication.</p>
<h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3>
<hr />
<p><em>Def</em>: For a complex matrix <span
class="math inline">A</span>, an <strong>eigenvalue</strong> <span
class="math inline">\lambda \in \mathbb{C}</span> and
<strong>eigenvector</strong> <span class="math inline">v \neq 0</span>
satisfy <span class="math display">
  Av = \lambda v.
</span></p>
<p><em>Def</em>: Furthermore, an <strong>eigenspace</strong> is the span
of all eigenvectors correspnding to a single eigenvalue, the
<strong>spectrum</strong> of <span class="math inline">A</span>, <span
class="math inline">\mathop{\mathrm{Spec}}(A)</span> is the set of all
eigenvalues of <span class="math inline">A</span>, and the
<strong>spectral radius</strong> is <span class="math inline">\rho(A) =
\max_{\lambda \in \mathop{\mathrm{Spec}}(A)} |\lambda| =
|\lambda_{\text{max}}|</span>. Sometimes we will call this the top
eigenvector/eigenvalue.</p>
<p>As a convention, usually we implicitly order eigenvalues, e.g. <span
class="math inline">|\lambda_1| \geq |\lambda_2| \geq \cdots \geq
\lambda_n</span>.</p>
<p><em>Def</em>: More generally, if <span class="math inline">v</span>
is a eigenvector of <span class="math inline">A^T</span>, we say that
<span class="math inline">v</span> is a <strong>left
eigenvector</strong> of <span class="math inline">A</span> (and thus
usual eigenvectors are <strong>right eigenvectors</strong>).</p>
<p>There are a few warnings about these things:</p>
<ul class="incremental">
<li>real matrices may have complex eigenvectors;</li>
<li>we normally normalize eigenvectors to have unit length;</li>
<li>left and right eigenvectors are usually not the same.</li>
</ul>
<p><em>Def</em>: A square matrix <span class="math inline">A \in
\mathbb{C}^{n \times n}</span> is diagonalizable if it is similarly to a
diagonal matrix.</p>
<p><strong>Prop</strong>: <span class="math inline">A</span> is
diagonalizable if and only if it has <span class="math inline">n</span>
linearly independent eigenvectors.</p>
<p><em>Proof</em>: The eigenvectors form a basis by intertibility;
change basis from the eigenvectors, scale by eigenvalues, and change
basis back. In particular, let <span class="math inline">A \in
\mathbb{C}^{n \times n}</span> have eigenvalues <span
class="math inline">\lambda_1, \dots, \lambda_n</span>, with
corresponding eigenvectors <span class="math inline">v_1, \dots,
v_n</span>; set <span class="math inline">\Lambda =
\mathop{\mathrm{diag}}(\lambda_1, \dots, \lambda_n)</span> and <span
class="math inline">V = \left[\begin{matrix}v_1, \dots,
v_n\end{matrix}\right]</span>. Then <span class="math display">
  A = V \Lambda V^{-1}.
</span></p>
<p><em>Def</em>: The above <span class="math inline">A = X \Lambda
X^{-1}</span> is called the <strong>eigenvalue decomposition</strong>,
or <strong>EVD</strong>.</p>
<p><strong>Prop</strong>: The diagonal entries in a triangular matrix
are just the eigenvalues.</p>
<p><em>Def</em>: A matrix <span class="math inline">A \in \mathbb{C}^{n
\times n}</span> is <strong>normal</strong> if it commutes with its
adjoint <span class="math inline">A^*</span> (the conjugate transpose),
and <strong>unitary</strong> if <span class="math inline">AA^* = A^*A =
I</span>.</p>
<p><strong>Prop</strong>: <span class="math inline">A</span> is unitary
if and only if it’s columns (or rows) are orthonormal.</p>
<p><strong>Theorem</strong>: <span class="math inline">A \in
\mathbb{C}^{n \times n}</span> is normal if and only if it is unitarily
diagonalizable, e.g. if <span class="math inline">A = V\Lambda
V^*</span> with <span class="math inline">V</span> unitary.</p>
<p><em>Def</em>: <span class="math inline">A \in \mathbb{C}^{n \times
n}</span> is <strong>Hermitian</strong> if <span class="math inline">A^*
= A</span>.</p>
<p><strong>Theorem (Spectral Theorem)</strong>: <a
href="https://en.wikipedia.org/wiki/Spectral_theorem">Link</a> <span
class="math inline">A \in \mathbb{C}^{n \times n}</span> is Hermitian if
and only if <span class="math inline">A</span> is unitarily
diagonalizable with all real eigenvalues.</p>
<p><strong>Corollary</strong>: <span class="math inline">A \in
\mathbb{R}^{n \times n}</span> is symmetric if and only if it is
orthogonally diagonalizable with all eigenvalues real.</p>
<h4 id="jordan-canonical-form">Jordan Canonical Form</h4>
<p><em>Def</em>: <a
href="https://en.wikipedia.org/wiki/Jordan_normal_form">Link</a> Any
matrix can be written in <strong>Jordan canonical form</strong>, e.g.
<span class="math display">
  A = XJX^{-1}
</span> where <span class="math inline">J</span> is nonzero only on the
diagonal and the superdiagonal, which has values only in <span
class="math inline">\{0, 1\}</span>, such as <span class="math display">
  J = \begin{bmatrix}
    \lambda_1 &amp; 1         &amp;           &amp;   &amp; &amp; &amp;
\\
              &amp; \lambda_1 &amp; 1         &amp;   &amp; &amp; &amp;
\\
              &amp;           &amp; \lambda_1 &amp; 0 &amp; &amp; &amp;
\\
              &amp;           &amp; &amp; \lambda_2 &amp; 0 &amp; &amp;
\\
              &amp;           &amp;         &amp;  &amp; \lambda_3 &amp;
1 &amp; \\
              &amp;           &amp;         &amp;  &amp; &amp;
\lambda_3  &amp; \\
  \end{bmatrix}
</span> for example.</p>
<p>The way it’s written above is not by coincidence: you can permute
everything so that <span class="math inline">J</span> is composed of
Jordan blocks, which have the same entry all the way down the diagonal
and have a superdiagonal of only ones, e.g. <span class="math display">
  J = \begin{bmatrix}
    J_1 &amp; &amp; &amp; \\
    &amp; J_2 &amp; &amp; \\
    &amp; &amp; \ddots &amp; \\
    &amp; &amp; &amp; J_k \\
  \end{bmatrix}
</span> where each <span class="math inline">J_i = \lambda I + N</span>,
where <span class="math inline">N</span> has all zero entries except on
the superdiagonal, on which it is always one.</p>
<p>Unfortunately, the JCF is pretty useless in application.</p>
<p><strong>Theorem (Golub-Wilkinson)</strong>: The Jordan canonical form
cannot be computed in finite precision.</p>
<h4 id="spectral-radius">Spectral Radius</h4>
<p>Let’s return to the spectral radius, <span class="math display">
  \rho(A) = \max_{\lambda \in \mathop{\mathrm{Spec}}(A)} |\lambda|.
</span></p>
<p>Any nilpotent matrix shows that <span class="math inline">\rho</span>
is not a norm, but the following is true.</p>
<p><strong>Prop</strong>: If <span class="math inline">\| \cdot \|:
\mathbb{C}^{m \times m} \to \mathbb{R}</span> is any consistent matrix
norm, then <span class="math inline">\rho(A) \leq \| A \|</span>.</p>
<p><em>Proof</em>: Look at the norm of the image of a top
eigenvector.</p>
<p><strong>Theorem</strong>: Given any <span class="math inline">A \in
\mathbb{C}^{n \times n}</span>, any positive <span
class="math inline">\epsilon</span>, there is a consistent norm (in
fact, an operator norm) <span class="math inline">\| \cdot \|:
\mathbb{C}^{n \times m} \to \mathbb{R}</span> such that <span
class="math display">
  \| A \| \leq \rho(A) + \epsilon.
</span></p>
<p><strong>Prop</strong>: Given any matrix norm <span
class="math inline">\| \cdot \|: \mathbb{C}^{n \times m} \to R</span>,
we have <span class="math display">
  \rho(A) = \lim_{k \to \infty} \| A^k \|^{1/k}.
</span></p>
<p><strong>Lemma</strong>: <span class="math inline">\lim_{k \to \infty}
A^k = 0</span> if and only if <span class="math inline">\rho(A) &lt;
1</span>.</p>
<p><em>Proof</em>: <span class="math inline">(\implies)</span> Set <span
class="math inline">\lambda</span> to be a top eigenvalue of <span
class="math inline">A</span>, and <span class="math inline">x</span> a
corresponding eigenvector. Then, <span class="math inline">A^k x =
\lambda^k x</span>. Send <span class="math inline">k \to \infty</span>
and conclude.</p>
<p><span class="math inline">(\impliedby)</span> By the above theorems,
there is some operator norm <span class="math inline">\| \cdot \|</span>
such that <span class="math inline">\|A\| \leq \rho(A) + \epsilon &lt;
1</span>. Send <span class="math inline">k \to \infty</span> and use the
fact that operator norms imply <span class="math inline">\| A^k \| \leq
\|A\|^k</span> and win.</p>
<h4 id="finding-eigenvalues">Finding Eigenvalues</h4>
<p><strong>Theorem (Gershgorin Circle Theorem)</strong>: <a
href="https://en.wikipedia.org/wiki/Gershgorin_circle_theorem">Wikipedia</a>
Let <span class="math inline">A \in \mathbb{C}^{n \times n}</span>, with
entries <span class="math inline">a_{ij}</span> and for <span
class="math inline">1 \leq i \leq n</span> set <span
class="math inline">r_i = \sum_{j\neq i} |a_{ij}|</span>. Then, every
eigenvalue of <span class="math inline">A</span> lies within the union
of the Gershgorin discs <span class="math display">
  G_i = \left\{ z \mid z \in \mathbb{C}, |z - a_{ii}| \leq \sum_{j \neq
i} |a_{ij}| \right\}
</span> and the number of eigenvalues in each connected component is
equal to the number of Gershgorin disks that constitute that
component.</p>
<p><em>Proof</em>: If <span class="math inline">A \in \mathbb{C}^{n
\times n}</span> is strictly diagonally dominant, e.g. <span
class="math display">
  |a_{ii}| &gt; \sum_{j \neq i}|a_{ij}|
</span> for all <span class="math inline">1 \leq i \leq n</span>, then
<span class="math inline">A</span> is invertible. To see this, take any
<span class="math inline">x \in \ker(A)</span> so that <span
class="math inline">\sum_{j=1}^n a_{ij}x_i = 0</span>. But look at the
index <span class="math inline">k</span> that witnesses <span
class="math inline">\| x \|_\infty</span>: <span class="math display">
  -a_{kk}x_k = \sum_{j \neq k} a_{kj}x_j \implies |a_{kk}||x_k| \leq
\sum_{j \neq k} |a_{kj}||x_k|
</span> so <span class="math inline">x = 0</span> and <span
class="math inline">\ker(A)</span> is trivial.</p>
<p>We proceed to prove that any <span class="math inline">z \notin
\bigcup_{i=1}^n G_i</span> cannot be an eigenvalue by showing that <span
class="math inline">A - zI</span> is invertible. But this is clear,
since <span class="math inline">A - zI</span> in that case is strictly
diagonally dominant.</p>
<p><em>Def</em>: <a
href="https://en.wikipedia.org/wiki/Schur_decomposition">Wikipedia</a>
The <strong>Schur decomposition</strong> of a matrix <span
class="math inline">A</span> is <span class="math inline">A =
QUQ^*</span> such that <span class="math inline">Q</span> is unitary and
<span class="math inline">U</span>, the <strong>Schur form</strong> of
<span class="math inline">A</span> is upper triangular.</p>
<p>Note that you can read off the eigenvalues of <span
class="math inline">A</span> from the diagonal of the Schur form, which
is numerically stable to compute.</p>
<p>As an aside, how one numerically finds the roots of a polynomial
<span class="math inline">p(x) = \sum_{k=0}^m c_kx^k</span> is by
forming the companion matrix <span class="math display">
  A = \begin{bmatrix}
    0 &amp; \cdots &amp; 0 &amp; -c_0 \\
    1 &amp; \ddots &amp;0 &amp; -c_1 \\
    \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
    0 &amp; \cdots &amp; 1 &amp; -c_{d-1}
  \end{bmatrix}.
</span></p>
<h3 id="singular-value-decomposition">Singular Value Decomposition</h3>
<hr />
<p><em>Def</em>: <a
href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Link</a>
The <strong>SVD decomposition</strong> of a real (resp. complex) matrix
<span class="math inline">A \in \mathbb{C}^{m \times n}</span>) is <span
class="math display">
  A = U \Sigma V^*
</span> where <span class="math inline">U, V</span> are orthogonal
(resp. unitary) and <span class="math inline">\Sigma</span> is diagonal
(on the shorter diagonal) with nonnegative entries; that is, if <span
class="math inline">m &gt; n</span>, then <span class="math display">
  \Sigma = \left[\begin{matrix}\mathop{\mathrm{diag}}(\sigma_1, \dots,
\sigma_n) \\ \mathbf 0\end{matrix}\right] \in \mathbb{R}^{m \times
n}_{\geq 0}
</span> and if <span class="math inline">m &lt; n</span>, <span
class="math display">
  \Sigma = \left[\begin{matrix}\mathop{\mathrm{diag}}(\sigma_1, \dots,
\sigma_n) &amp; \mathbf 0\end{matrix}\right] \in \mathbb{R}^{m \times
n}_{\geq 0}
</span></p>
<p><em>Def</em>: We may arrange <span class="math inline">\sigma_{1}
\geq \sigma_2 \geq \dots \geq \sigma_{\min(m, n)} \geq 0</span>; we then
call top few singular values the principal singular values. The columns
of <span class="math inline">U</span> are left singular vectors, and the
columns of <span class="math inline">V</span> are right singular
vectors.</p>
<p><strong>Prop</strong>: The largest <span class="math inline">r</span>
for which <span class="math inline">r &gt; 0</span> is the rank of <span
class="math inline">A</span>.</p>
<p><em>Def</em>: We sometimes use the <strong>reduced SVD</strong>,
which is the same as the normal SVD but we drop the extra rows of <span
class="math inline">\Sigma</span> to force it to be a square diagonal
matrix in <span class="math inline">\mathbb{C}^{\min\{m, n\} \times
\min\{m, n\}}</span>. Then, <span class="math display">
  A = \sum_{k=1}^r \sigma_k u_r v_r^*
</span> where <span class="math inline">u_r, v_r</span> are the columns
of <span class="math inline">U, V</span> in the reduced SVD and <span
class="math inline">r = \operatorname{rank}(A)</span>, or the number of
nonzero singular values.</p>
<p><em>Def</em>: In a similar vein, the <strong>condensed SVD</strong>
is attained by removing all of the nonzero rows/columns of <span
class="math inline">\Sigma</span>, so <span class="math inline">\Sigma
\in \mathbb{C}^{r \times r}</span>.</p>
<p><strong>Theorem</strong>: Given any <span class="math inline">A \in
\mathbb{C}^{m \times n}</span>, there are <span class="math inline">U
\in \mathbb{C}^{m \times m}</span>, <span class="math inline">V \in
\mathbb{C}^{n \times n}</span>, and <span class="math display">
  \Sigma = \mathop{\mathrm{diag}}(\sigma_1, \dots, \sigma_r, 0, \dots,
0) \in \mathbb{R}_{\geq 0}^{m \times n}
</span> such that <span class="math inline">U, V</span> are unitary and
<span class="math inline">A = U \Sigma V^*</span>.</p>
<p><em>Proof</em>: Form the matrix <span class="math display">
  W = \left[\begin{matrix}0 &amp; A \\ A^* &amp; 0\end{matrix}\right]
</span> which is Hermitian. The spectral theorem gives an EVD, and if
you look carefully you get the compact SVD: <span class="math display">
  W = \left[\begin{matrix} U_r &amp; U_r \\ V_r &amp; -V_r
\end{matrix}\right] \left[\begin{matrix}
  \sigma_1 &amp; &amp; &amp; &amp; &amp;  &amp;\\
   &amp; \sigma_2 &amp; &amp; &amp; &amp; &amp;\\
   &amp; &amp; \ddots &amp; &amp; &amp; &amp;\\
   &amp; &amp;  &amp; \sigma_r &amp; &amp; &amp;\\
   &amp; &amp;  &amp; &amp; -\sigma_1 &amp; &amp; \\
   &amp; &amp;  &amp; &amp; &amp; \ddots &amp; \\
   &amp; &amp;  &amp; &amp; &amp; &amp; -\sigma_r\\ \end{matrix}\right]
  \left[\begin{matrix} U_r &amp; U_r \\ V_r &amp; -V_r
\end{matrix}\right]^*.
</span></p>
<p>Singular values have a nice property: we have for any singular value
<span class="math inline">\sigma</span> and left singular vector <span
class="math inline">u</span> and right singular vector <span
class="math inline">v</span>, <span class="math inline">Av = \sigma
u</span> and <span class="math inline">A^*v = \sigma v</span>; thus
<span class="math inline">u, v</span> are eigenvalues of <span
class="math inline">AA^*</span> and <span
class="math inline">A^*A</span> respectively.</p>
<h3 id="applications-of-the-svd">Applications of the SVD</h3>
<hr />
<p>We can read off many important quantities/spaces from the SVD. Let
<span class="math inline">A = U \Sigma V^*</span> be a SVD, with only
<span class="math inline">r</span> nonzero singular values.</p>
<ul class="incremental">
<li>The rank is the number of nonzero singular values.</li>
<li>The absolute value of the determinant is the product of the singular
values.</li>
<li>The two norm is the maximum singular value.</li>
<li>Set <span class="math inline">\sigma = (\sigma_1, \dots,
\sigma_n)</span>; then the Frobenius norm of <span
class="math inline">A</span> is <span class="math inline">\| \sigma
\|_2</span>, and we call the general case of this the <strong>Schatten
norm</strong>, e.g. <span class="math inline">\| A \|_{S, p} = \| \sigma
\|_p</span>; the case of <span class="math inline">p = 1</span> is also
called the <strong>nuclear norm</strong>.</li>
<li>The Ky Fan <span class="math inline">(p, k)</span> norm for an
integer <span class="math inline">1 \leq k \leq \infty</span> is <span
class="math display">
  \| A \|_{S, p, k} = \left( \sum_{i=1}^p \sigma_i^p \right)^{1/p}.
</span></li>
<li>The kernel of <span class="math inline">A = U \Sigma V^*</span> is
spanned by <span class="math inline">v_{r+1}, \dots, v_n</span>.</li>
<li>The image of <span class="math inline">A</span> is <span
class="math inline">u_1, \dots, u_r</span>.</li>
<li>The kernel of <span class="math inline">A^T</span> (this is the
cokernel) is spanned by <span class="math inline">v_{1, \dots,
v_r}</span>.</li>
<li>The image of <span class="math inline">A^T</span> is spanned by
<span class="math inline">u_{r+1}, \dots, u_m</span>.</li>
</ul>
<p>You can also solve fundamental problems. For example, if you consider
the linear system <span class="math inline">Ax = b</span>, it might not
have exactly one solution, so we can translate this into the least
squares system <span class="math inline">\min_{x \in \mathbb{R}^n} \| Ax
- b \|_2^2</span>; this still might not have a unique solution when
<span class="math inline">A</span> is singular, so we consider the
<strong>minimum norm least squares</strong> problem: <span
class="math inline">\min \{ \| x \|_2 \mid x \in
\mathop{\mathrm{argmin}}\| A_x - b \|_2 \}</span>.</p>
<p>SVD solves all of these simultaneously; set <span
class="math inline">A = U \Sigma V^T</span>; this means that we want
<span class="math inline">\Sigma V^T x = U^T b</span>; equivalently, we
just need to solve <span class="math inline">\Sigma y = c</span>, where
<span class="math inline">b = Uc, x = Vy</span>. But this is trivial
since <span class="math inline">\Sigma</span> is diagonal.</p>
<p>We could define the pseudoinverse of <span
class="math inline">A</span> to be <span class="math inline">A^+</span>
to be the matrix <span class="math inline">A^+</span> that sends any
<span class="math inline">b</span> to the minimum norm least squares
solution of <span class="math inline">Ax = b</span>.</p>
<p><strong>Prop</strong>: <span class="math inline">A^+</span> is just
<span class="math inline">V \Sigma^{-1} U^*</span>, where by <span
class="math inline">\Sigma^{-1}</span> is just gotten by flipping all
the nonzero diagonal elements.</p>
<p><em>Proof</em>: Clear.</p>
<p><em>Def</em>: In general, the <strong>pseudoinverse</strong> or
<strong>Moore-Penrose</strong> inverse of <span class="math inline">A
\in K^{m \times n}</span> is a matrix <span class="math inline">X \in
K^{n \times m}</span> that satisfies</p>
<ul class="incremental">
<li><span class="math inline">(AX)^{*} = AX</span>,</li>
<li><span class="math inline">(XA)^* = XA</span>,</li>
<li><span class="math inline">AXA = A</span>,</li>
<li><span class="math inline">XAX = X</span>.</li>
</ul>
<p><strong>Prop</strong>: The above gives a unique matrix and is in fact
the same as <span class="math inline">A^+</span> from earlier.</p>
<p><strong>Prop</strong>: The following statements are true.</p>
<ul class="incremental">
<li><span class="math inline">A^+A</span> and <span
class="math inline">AA^+</span> are not necessarily <span
class="math inline">I</span>.</li>
<li>If <span class="math inline">A</span> has full column rank <span
class="math inline">A^+ = (A^*A)^{-1}A</span>.</li>
<li>If <span class="math inline">A</span> has full row rank, <span
class="math inline">A^+ = A^*(AA^*)^{-1}</span>.</li>
</ul>
<h4 id="ridge-regressions">Ridge Regressions</h4>
<p>Suppose that we are looking for <span class="math display">
  \min \|Ax - b\|^2_2 \text{ such that } \|x\|^2_2 \leq \alpha^2
</span> which is a ridge regression. Remember that the pseudoinverse
<span class="math inline">A^+</span> gives that <span
class="math inline">A^+b</span> is the minimal norm solution; so if
<span class="math inline">\|A^+b\| \leq \alpha</span>, this works. On
the other hand, if we have <span class="math inline">\|A^+b\| &gt;
\alpha</span>, we first use the fact that the above optimization reduces
to one on the boundary <span class="math inline">\|x\|_2 =
\alpha</span>. Then take the SVD decomposition <span
class="math inline">A = U\Sigma V^T</span>, and look at the Lagrange
multiplier condition <span class="math display">
  A^TAx - A^Tb - \lambda x = 0 \implies x = (A^TA - \lambda I)^{-1}A^Tb
</span> and thus, setting <span class="math inline">c = U^b</span>,
<span class="math display">
  \begin{align*}
    \alpha^2 &amp; = x^Tx \\
             &amp; = b^TA(A^TA- \lambda I)^{-2}A^Tb \\
             &amp; = c^T \Sigma(\Sigma^T\Sigma + \lambda
I)^{-2}\Sigma^Tc \\
             &amp; = \sum_{i=1}^r \frac{c_i^2\sigma_i^2}{(\sigma_i^2 +
\lambda)^2} = f(\lambda).
  \end{align*}
</span> Then, solve <span class="math inline">f(\lambda) =
\alpha^2</span> to get a solution (or really, <span
class="math inline">f(\lambda)^{-1} = \alpha^{-2}</span> since most
univariate root finders are bad when things go to infinity). In fact,
there are many solutions, but the one we are looking for is actually the
biggest solution.</p>
<h4 id="matrix-approximation-problems">Matrix Approximation
Problems</h4>
<p>Suppose that <span class="math inline">A \in \mathbb{R}^{n \times
n}</span>; then, we want to solve <span class="math display">
  \min_{X^T = X} \|A - X\|_F
</span> or <span class="math display">
  \min_{X^TX = 1} \|A - X\|_F
</span> or if <span class="math inline">A \in \mathbb{R}^{m \times
n}</span>, <span class="math display">
  \min_{\mathop{\mathrm{rank}}(X) \leq r} \|A - X\|_F.
</span> The last problem is very important since rounding error sends
every matrix to an invertible matrix, so the above approximates a
solution to this problem.</p>
<p>Set <span class="math display">
  A = \frac{A + A^T}{2} + \frac{A - A^T}{2} = X + Y;
</span> then, <span class="math inline">X = \frac{A + A^T}{2}</span> is
the solution to the first problem. Further, we can compute <span
class="math display">
  \mathop{\mathrm{tr}}(X^TY) = 0
</span> and so <span class="math display">
  \|A\|^2 = \|X + Y\|^2 = \mathop{\mathrm{tr}}(X^TX) +
2\mathop{\mathrm{tr}}(X^TY) + \mathop{\mathrm{tr}}(Y^TY) = \|X\|^2 +
\|Y\|^2.
</span> In general, we see <span class="math inline">\mathbb{R}^{n
\times n} = S^2(\mathbb{R}^n) \oplus \Lambda^2(\mathbb{R}^n)</span>,
e.g. the space of all matrices is the direct sum of symmetric and skew
symmetric matrices.</p>
<p>In the second problem, take the SVD <span class="math inline">A =
U\Sigma V^T</span> and <span class="math inline">Z = U^T X V</span> so
that <span class="math display">
\begin{align*}
  \min_{X^TX = I}\|U \Sigma V^T - X\|_F^2 &amp; = \min_{X^TX =
I}\|\Sigma - U^TXV\|^2_F \\
                                          &amp; = \min_{Z^TZ = I}\|
\Sigma - Z \|^2_F \\
                                          &amp; = \max_{Z^TZ = I} \|
\Sigma \|_F^2 - 2 \mathop{\mathrm{tr}}(\Sigma^T Z) + \|Z\|^2_F
                                          &amp; = \max_{Z^TZ = I} \|
\Sigma \|_F^2 - 2 \mathop{\mathrm{tr}}(\Sigma^T Z) + n
\end{align*}
</span> so we just need to maximize <span
class="math inline">\mathop{\mathrm{tr}}(\Sigma^T Z) = \sum_{i=1}^n
\sigma_i z_ii \leq \sum_{i=1}^n \sigma_i</span>; but this is attained
when <span class="math inline">Z = I</span>, so we need <span
class="math inline">X = UV^2</span>.</p>
<p>The solution to the last problem is called the Eckhart-Young
theorem.</p>
<p><strong>Theorem (Eckhart-Young)</strong>: Let <span
class="math inline">A \in \mathbb{R}^{m \times n}</span> and let <span
class="math inline">A = U \Sigma V^T</span> be the SVD. Then the
solution to <span class="math inline">\min_{\mathop{\mathrm{rank}}(X)
\leq r} \|A - X\|_2</span> is given by dropping all but the first <span
class="math inline">r</span> entries in <span
class="math inline">\Sigma</span>.</p>
<p><em>Proof</em>: Suppose not. Then let the solution be called <span
class="math inline">B</span>. Then <span class="math display">
  \|A - X\|_2 = \|U \mathop{\mathrm{diag}}(0, \dots, 0, \sigma_{r+1},
\dots, \sigma_{\min(m, n)}) V^T\|_2 = \sigma_{r + 1}.
</span> However, the nullity of <span class="math inline">B</span> is at
least <span class="math inline">n - r</span>; but if we take <span
class="math inline">W</span> to be the span of the first <span
class="math inline">r+1</span> right singular vectors, then any <span
class="math inline">w \in W</span> is <span class="math display">
  w = V_{r+1}\alpha
</span> where <span class="math inline">V_{r+1}</span> is the first
<span class="math inline">r+1</span> columns of <span
class="math inline">X</span> and <span class="math inline">\alpha</span>
is some vector in <span class="math inline">\mathbb{R}^{r+1}</span>.
Then, <span class="math display">
  \|Aw\|^2 = \| U\Sigma V^T V_{r+1}\alpha \|_2^2 =
\sum_{i=1}^{r+1}\sigma_i^2 \|\alpha_i\|^2 \geq \sigma_{r+1}^2 \|w\|^2.
</span> But if we pick some <span class="math inline">w \in
\ker(B)</span>, then <span class="math inline">Aw = (A - B)w</span>, so
<span class="math display">
  \|Aw\|_2 \leq \|A - B\|\|w\|_2 &lt; \sigma_{r+1}\|w\|_2
</span> and since the ranks sum above <span
class="math inline">r</span>, we have a contradiction.</p>
<p>The Eckhart-Young-Mirsky theorem extends this to all unitarily
invariant norms.</p>
<h4 id="total-least-squares-errors-in-variables-regression">Total Least
Squares / Errors in Variables Regression</h4>
<p>Consider a linear system <span class="math inline">Ax = b</span> with
<span class="math inline">A</span> having full column rank. Then, if
this system is not consistent, then we can “solve it” by taking <span
class="math inline">Ax= b + r</span> for some minimal <span
class="math inline">r</span>, which is just least squares;
alternatively, we can take <span class="math inline">(A + E)x =
b</span>, which is the <strong>data least squares</strong>; if we take
both, we get the <strong>total least squares</strong> <span
class="math inline">(A + E)x = b + r</span>.</p>
<p>In this last problem, we are minimizing <span
class="math inline">\|E\|_F^2 + \|r\|_2^2</span>; take <span
class="math inline">C = \left[\begin{matrix}A &amp;
b\end{matrix}\right]</span> and <span class="math inline">F =
\left[\begin{matrix}E &amp; b\end{matrix}\right]</span>. Then we can
restate the constraint to <span class="math inline">(C +
F)\left[\begin{matrix}x \\ -1\end{matrix}\right] = 0</span>. Then either
<span class="math inline">\mathop{\mathrm{rank}}(C) = n</span>, in which
case <span class="math inline">b</span> is in the span of <span
class="math inline">A</span> and we may take <span class="math inline">E
= 0, r = 0</span>, or <span
class="math inline">\mathop{\mathrm{rank}}(C) = n + 1</span>. In the
latter case, the kernel of <span class="math inline">C + F</span> is
nontrivial, and so <span class="math inline">\mathop{\mathrm{rank}}(C +
F) \leq n</span>.</p>
<p>Taking the SVD of both <span class="math inline">C</span> and <span
class="math inline">C + F</span>, we must have that <span
class="math display">
  F = U \mathop{\mathrm{diag}}(0, \dots, 0, \sigma_{n+1}, 0, \dots, 0)
V^T.
</span> Solve for <span class="math inline">z</span> in <span
class="math inline">(C + F)z = 0</span>, and take a solution so that the
last entry is <span class="math inline">-1</span>.</p>
<h3 id="floating-point-arithmetic">Floating Point Arithmetic</h3>
<p><em>Def</em>: We have a few different types of errors; let <span
class="math inline">x</span> be the real solution and <span
class="math inline">\hat x</span> the computed solution.</p>
<ul class="incremental">
<li>The <strong>forward error</strong> is <span
class="math inline">\|\hat x - x\|</span>,</li>
<li>the <strong>relative error</strong> is <span
class="math inline">\frac{\|\hat x - x\|}{\|x\|}</span>,</li>
<li>the <strong>pointwise error</strong> is <span
class="math inline">\left\| \frac{x_\cdot - \hat x_{\cdot}}{x_i}
\right\|</span>.</li>
</ul>
<p>Error is inherent in floating point arithmetic. If there are <span
class="math inline">n</span>-bits, there can only be <span
class="math inline">2^n</span> real numbers that are representable; the
way this works is by taking each number to be of the form <span
class="math display">
    \begin{matrix} \pm &amp; e_1 &amp; e_2 &amp; \dots &amp; e_l &amp;
a_1 &amp; a_2 &amp; \dots &amp; a_k\end{matrix} = \pm a_1.a_2a_3\dots
a_k \cdot 2^{e_1e_2\dots e_l}
</span> where the <span class="math inline">e_i</span> are called the
exponent and the <span class="math inline">a_i</span> are called the
mantissa.</p>
<p>The exponent is stored as two’s complement, and is computed as <span
class="math display">
    e = -e_1\cdot 2^{l - 1} + \sum_{k=1}^{l} e_{k+1}\cdot 2^{l - k}.
</span></p>
<p>If <span class="math inline">a_1 = 1</span>, it is called normal, and
if <span class="math inline">a_1 = 0</span>, it is called subnormal (and
since they are in some ways pathological, we ignore them for now).</p>
<p>Double precision corresponds to <span class="math inline">n =
64</span>, <span class="math inline">l = 11</span>, <span
class="math inline">k = 52</span>. This includes numbers from <span
class="math inline">2^{-1024}</span> to <span
class="math inline">2^{1024}</span>.</p>
<p>Let <span class="math inline">F</span> be the set of floating point
numbers. We have some rounding scheme <span
class="math inline">\mathop{\mathrm{fl}}: \mathbb{R}\to F</span> and
some machine <span class="math inline">\epsilon</span>, <span
class="math inline">\epsilon_{m}</span> such that <span
class="math display">
    \epsilon_m = \inf \{ x \in \mathbb{R}\mid x &gt; 0,
\mathop{\mathrm{fl}}(1 + x) \neq 1 \}.
</span> In double precision, <span class="math inline">\epsilon_m =
2^{-52}</span>.</p>
<p><strong>Theorem (Kahan)</strong>: For any <span class="math inline">x
\in [-2^{M}, -2^{m}] \cup [2^m, 2^M]</span>, there is <span
class="math inline">x&#39; \in F</span> such that <span
class="math inline">|x - x&#39;| \leq \epsilon_m |x|</span>.
Furthermore, for any <span class="math inline">x, y \in F</span>,</p>
<ul class="incremental">
<li><span class="math inline">\mathop{\mathrm{fl}}(x \pm y) = (x \pm
y)(1 + \epsilon_1)</span> where <span class="math inline">|\epsilon_1|
\leq \epsilon_m</span>,</li>
<li><span class="math inline">\mathop{\mathrm{fl}}(xy) = xy(1 +
\epsilon_2)</span> where <span class="math inline">|\epsilon_2| \leq
\epsilon_m</span>,</li>
<li><span class="math inline">\mathop{\mathrm{fl}}(x/y) = x/y \cdot (1 +
\epsilon_3)</span> where <span class="math inline">|\epsilon_3| \leq
\epsilon_m</span>.</li>
</ul>
<p>Unfortunately, <span class="math inline">F</span> is terrible
otherwise: it is not commutative, not associative, and not distributive.
We have all types of errors here: for example, round off error is stuff
like <span class="math inline">\mathop{\mathrm{fl}}(1.1112 \times 10^5)
= 1.111 \times 10^5</span>.</p>
<p>There is also overflow and underflow: when your computations exceed
the defined limits of the floating point standard; also cancellation
error: double precision claims that <span class="math display">
    844487^5 + 1288439^5 - 1318202^5 = 0
</span> which is about 200 billion off the real answer, but this fits
within the non-significant bits of the floating point
representation.</p>
<h4 id="avoiding-floating-point-error">Avoiding Floating Point
Error</h4>
<p>To compute <span class="math inline">\|x\|_2</span>, you compute
<span class="math inline">\|x\|_\infty</span>, compute <span
class="math inline">y = \frac{x}{\|x\|_\infty}</span>, and then finally
<span class="math inline">\|x\|_2 = \|x\|_\infty\|y\|</span>. The reason
to do this is to avoid underflow/overflow.</p>
<p>As another example, consider the sample variance; <span
class="math display">
    s^2 = \frac{1}{n-1} \left( \sum_{i=1}^n x_i^2 - n^{-1} \left(
\sum_{i=1}^n x_i \right)^2 \right)
</span> is a terrible formula, since it suffers from cancellation error;
but <span class="math display">
    s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)^2
</span> is fine. Similarly, <span class="math inline">x^2 - y^2</span>
is bad, but <span class="math inline">(x + y)(x - y)</span> is good. As
another example, you would rather compute <span class="math display">
    x_1 = \frac{-b + \operatorname{sign}(b) \sqrt{b^2 - 4ac}}{2a}
</span> and <span class="math inline">x_2 = \frac{c}{ax_1}</span>.</p>
<h3 id="conditioning">Conditioning</h3>
<p>Conditioning will be a property of the problem.</p>
<p>Suppose you have some problem, such as finding a solution to a linear
system <span class="math inline">Ax = b</span> for some invertible <span
class="math inline">A</span>. One thing that we can do is perturb <span
class="math inline">A</span> by a little bit, e.g. <span
class="math display">
    (A + \Delta A)(x + \Delta x) = b + \Delta b;
</span> then we want to bound <span class="math inline">\|\Delta x\| =
\|\hat x - x\|</span>. A rough answer in this case is possible; in
particular suppose <span class="math inline">\Delta A = 0</span>, so
that <span class="math display">
    \|\Delta x\| = \|A^{-1}\Delta b\|.
</span> Further, <span class="math inline">\|b\| \leq \|A\|\|x\|</span>,
so <span class="math inline">\frac{1}{\|x\|} \leq
\frac{\|A\|}{\|b\|}</span> and <span class="math display">
    \frac{\|\Delta x\|}{\|x\|} \leq \|A\|\|A^{-1}\|\frac{\|\Delta
b\|}{\|b\|}.
</span> The quantity <span class="math inline">\kappa(A) =
\|A\|\|A^{-1}\|</span> is so important that it is called the condition
number of <span class="math inline">A</span>. In the case of the two
norm, it is called the spectral condition number.</p>
<p>Now if the error is instead in <span class="math inline">A</span>, we
have that <span class="math display">
    \frac{\|\Delta x\|}{\|x\|} = \frac{\kappa(A)\frac{\|\Delta
A\|}{\|A\|}}{1 - \kappa(A)\frac{\|\Delta A\|}{\|A\|}}.
</span> Most generally, we have <span class="math display">
    \frac{\|\Delta x\|}{\|x\|} \leq \frac{\kappa(A)\left(\frac{\|\Delta
A\|}{\|A\|} + \frac{\|\Delta b\|}{\|b\|}\right)}{1 -
\kappa(A)\frac{\|\Delta A\|}{\|A\|}}.
</span> Thus, if <span class="math inline">\frac{\|\Delta A\|}{\|A\|},
\frac{\|\Delta b\|}{\|b\|} \leq \epsilon</span> then <span
class="math display">
    \frac{\|\Delta x\|}{\|x\|} \leq \frac{2\epsilon}{1 - \rho}\kappa(A)
</span> where <span class="math inline">\rho = \kappa(A)\frac{\|\Delta
A\|}{\|A\|}</span>.</p>
<p><em>Def</em>: Given any norm <span class="math inline">\|\cdot \|:
\mathbb{C}^{m \times n} \to \mathbb{R}</span>, the <strong>condition
number of a matrix</strong> <span class="math inline">A \in
\mathbb{C}^{m \times n}</span> is defined by <span class="math display">
    \kappa_{\|\cdot\|}(A) =  \|A\|\|A^+\|.
</span></p>
<p>In the case of the spectral norm, this is just the ratio of the
largest singular value to the smallest singular value.</p>
<p>We have that in general, <span class="math inline">1 \leq \kappa_2(A)
&lt; \infty</span>, and <span class="math inline">\kappa_2(A) = 1</span>
(perfectly conditioned) if and only if it is unitary.</p>
<p>Since you can get invertible matrices that we terribly conditioned,
we never care about the determinant or invertibility: we only ever
compute <span class="math inline">\mathop{\mathrm{rcond}}(A)</span> to
check for near-singularity, never the determinant.</p>
<p><em>Def</em>: A problem is <strong>well posed</strong> if a solution
exists and is unique, and <strong>ill-posed</strong> otherwise. An
<strong>instance</strong> of a problem is a selection of parameters that
describes the problem further.</p>
<p><em>Def</em>: The <strong>condition number of a problem</strong> is
the normalized reciprocal of the distance to the nearest ill-posed
instance.</p>
<p>For example, in the case of solving <span class="math inline">Ax =
b</span>, this will be <span class="math inline">\frac{\|A\|}{d(A,
M)}</span>, where <span class="math inline">M = \{X \in \mathbb{C}^{n
\times n} \mid \det(X) = 0 \}</span> is the ill-posed manifold.</p>
<p>(Quiz problem: show that <span class="math inline">d(A, M) =
\|A^{-1}\|^{-1}</span>).</p>
<p>Luckily, if <span class="math inline">A = U\Sigma V^T</span>, we can
see that <span class="math inline">\|\Delta A\| = \|\Delta
\Sigma\|</span> so the SVD is perfectly conditioned and so we will be
able to find it to machine precision.</p>
<p>In some sense, every problem gives rise to a map <span
class="math inline">f: X \to Y</span> from input data to an output
solution; when the problem is well posed this is a function. When <span
class="math inline">X, Y</span> have norms, then <span
class="math display">
    \kappa_f(x) = \lim_{\delta \to 0} \sup_{\operatorname{RelErr}(x)
&lt; \delta}\frac{\operatorname{RelErr}(f(x))}{\operatorname{RelErr}(x)}
</span> is the (relative) condition number. Recall that the relative
error is <span class="math display">
    \operatorname{RelErr}(x) = \frac{\|\Delta x\|}{\|x\|}.
</span> Then, we immediately see that <span class="math display">
    \operatorname{RelErr}(f(x)) \leq \kappa_f(x)
\operatorname{RelErr}(x) + o(\operatorname{RelErr}(x))
</span> or, with names, <span class="math display">
    \text{forward error} \lessapprox \text{condition number } \cdot
\text{ backward error}
</span></p>
<h3 id="stability">Stability</h3>
<p>Stability, on the other hand will be a property of an algorithm. As
above, each problem is some <span class="math inline">f:X \to Y</span>
sending inputs to solutions; on the other hand, we only have some
algorithm <span class="math inline">\hat f:X \to Y</span>.</p>
<p>Suppose that we have some backward error <span class="math inline">x
+ \Delta x</span> and some forward error <span class="math inline">y +
\Delta y</span>, such that <span class="math inline">\hat f(x + \Delta
x) = f(x) = y + \Delta y</span>.</p>
<p>For example, if <span class="math inline">A \in
\operatorname{GL}(n)</span>, then the solution to <span
class="math inline">Ax = b</span> is <span class="math inline">f(A, b) =
A^{-1}b</span>; for any algorithm <span class="math inline">\hat f(A, b)
= \hat x</span>, if we know <span class="math inline">A</span> exactly,
then the forward error is <span class="math display">
    \|f(A, b) - \hat f(A, b)\| = \|A^{-1} b - \hat x\|
</span> and the backward error is <span class="math display">
    \|A\hat{x} - b\|.
</span></p>
<p>If the problem is SVD, then we have <span class="math inline">f(A) =
(U, \Sigma, V)</span>; the forward errors are <span
class="math display">
    \|U - \hat U\|, \|V - \hat V\|, \|\Sigma - \hat \Sigma\|
</span> but the backward error is just <span class="math display">
    \|A - \hat U \hat \Sigma \hat V^T\|.
</span></p>
<p><em>Def</em>: We say an algorithm <span class="math inline">\hat
f</span> is <strong>backwards stable</strong>, if for any <span
class="math inline">x \in X</span>, the computed <span
class="math inline">\hat y = \hat f(x)</span> satisfies that <span
class="math display">
    \hat y = f(x + \Delta x), \ \  \|\Delta x\| \leq \delta \|x\|
</span> for <span class="math inline">\delta</span> small.</p>
<p><em>Def</em>: We say that <span class="math inline">\hat f</span> is
<strong>numerically stable</strong> if for any <span
class="math inline">x \in X</span>, the computed <span
class="math inline">\hat y = \hat f(x)</span> satisfies <span
class="math display">
    \hat y + \Delta y = f(x + \Delta x), \ \ \|\Delta x\| \leq \delta
\|x\|, \|\Delta y\| \leq \delta \|y\|
</span> for <span class="math inline">\delta, \epsilon</span> small.</p>
<h3 id="qr-decomposition">QR Decomposition</h3>
<p><em>Def</em>: Let <span class="math inline">A \in \mathbb{R}^{m
\times n}</span> with <span class="math inline">n \leq m</span>; then we
can find a decomposition <span class="math inline">A = QR</span>, where
<span class="math inline">Q \in O(m)</span> is orthogonal and <span
class="math inline">R \in \mathbb{R}^{m \times n}</span> is upper
triangular. In particular, we have <span class="math inline">R =
\left[\begin{matrix}R_1 \\ 0\end{matrix}\right]</span> where <span
class="math inline">R_1</span> is upper triangular in <span
class="math inline">\mathbb{R}^{n \times n}</span>. If <span
class="math inline">A</span> is of full column rank, then <span
class="math inline">R_1</span> is invertible. This is called the
<strong>full <span class="math inline">QR</span>
decomposition</strong>.</p>
<p><em>Def</em>: In the same setup as above, partition <span
class="math inline">A = \left[\begin{matrix}Q_1 &amp;
Q_2\end{matrix}\right]\left[\begin{matrix}R_1 \\ 0\end{matrix}\right] =
Q_1R_1</span>. This is the <strong>condensed <span
class="math inline">QR</span> decomposition</strong>.</p>
<p><em>Def</em>: In the same setup as above, if we take <span
class="math inline">\mathop{\mathrm{rank}}(A) = r</span>, then there is
a permutation matrix <span class="math inline">\Pi</span> such that
<span class="math display">
    A\Pi = Q \left[\begin{matrix}R_1 &amp; S \\ 0 &amp;
0\end{matrix}\right]
</span> and so <span class="math display">
    A = Q \left[\begin{matrix}R_2^T &amp; 0 \\ 0 &amp;
0\end{matrix}\right]Z^T \Pi^T
</span> where <span class="math inline">R_1 \in \mathbb{C}^{r \times
r}</span> and we get <span class="math inline">R_2</span> by doing the
full <span class="math inline">QR</span> decomposition on <span
class="math inline">\left[\begin{matrix}R_1^T \\ S^T\end{matrix}\right]
= Z \left[\begin{matrix}R_2 \\ 0\end{matrix}\right]</span>. This is the
<strong>rank-retaining <span class="math inline">QR</span>
decomposition</strong>.</p>
<p><em>Def</em>: For any <span class="math inline">A \in \mathbb{R}^{m
\times n}</span>, we may find <span class="math inline">A =
Q\left[\begin{matrix}L &amp; 0 \\ 0 &amp;
0\end{matrix}\right]U^T</span>. This is the <strong>complete orthogonal
decomposition</strong>.</p>
<h4 id="solving-linear-equations">Solving Linear Equations</h4>
<p>To solve <span class="math inline">Ax = b</span>, we can take <span
class="math inline">A = QR</span> so <span class="math inline">Rx =
Q^Tb</span> and solve using back substitution. Alternatively, let <span
class="math inline">A\Pi = LU</span>; then we solve <span
class="math inline">Ly = b</span> with back substitution, <span
class="math inline">Uz = y</span> with forward substitution, and set
<span class="math inline">x = \Pi z</span>.</p>
<p>Linearly constrained least squares problems are problems of the form
<span class="math display">
    \min\| Ax - b \|, \  \ \text{s.t. } C^Tx = d.
</span> where <span class="math inline">A \in \mathbb{R}^{m \times n}, b
\in \mathbb{R}^n, C \in \mathbb{R}^{n \times p}</span>, and <span
class="math inline">d \in \mathbb{R}^p</span>.</p>
<ul class="incremental">
<li><p>We could form the Lagrangian to get <span class="math display">
  \begin{cases}
      A^Ax - A^Tb + C\lambda = 0\\
      C^Tx - d = 0
  \end{cases}   
</span> which is a KKT constraint. Sometimes we care about <span
class="math inline">\lambda</span>, and this is fine (though <span
class="math inline">A^TA</span> can often be ill-conditioned).</p></li>
<li><p>Instead, we may use the QR decomposition: note that <span
class="math inline">A^TAx = A^Tb - C\lambda</span> and thus <span
class="math inline">x = \hat x - (A^TA)^{-1}C\lambda</span>, where <span
class="math inline">\hat x = \mathop{\mathrm{argmin}}\|Ax -
b\|_2</span>. Then, since <span class="math inline">C^Tx = d</span>, we
must have that <span class="math display">
  C^T(A^TA)^{-1}C\lambda = C^T\hat x - d.
</span> To avoid <span class="math inline">A^TA</span>, set <span
class="math inline">A = Q\left[\begin{matrix}R \\
0\end{matrix}\right]</span>, set <span class="math inline">W =
R^{-T}C</span> via backsubstitution, take the QR <span
class="math inline">W = Q_1R_1</span>, set <span
class="math inline">\eta = C^T \hat x - d</span> and finally solve <span
class="math inline">R_1^TR_1 \lambda = \eta</span> via
backsubstitution.</p></li>
<li><p>If we do not want to compute <span
class="math inline">\lambda</span>, if we take <span
class="math inline">p \leq n</span>, let <span class="math inline">C =
Q_2 \left[\begin{matrix}R_2 \\ 0\end{matrix}\right]</span>, so that
<span class="math inline">\left[\begin{matrix}R_2^T &amp;
0\end{matrix}\right]Q_2^Tx = d</span>. Thus, if we take <span
class="math inline">Q_2^Tx = \left[\begin{matrix}u \\
v\end{matrix}\right]</span>, we backsolve for <span
class="math inline">u</span> such that <span class="math inline">R_2^Tu
= d</span>, we know that <span class="math display">
  \|b - Ax\|^2 = \|b - AQ_2Q_2^Tx\|^2 = \left\|b -
\left[\begin{matrix}A_1 &amp; A_2\end{matrix}\right]
\left[\begin{matrix}u \\ v\end{matrix}\right]\right\|^2 = \|b - A_1u -
A_2v\|_2^2.
</span></p></li>
</ul>
<h4 id="computing-qr-lu-co-decompositions">Computing QR, LU, CO
Decompositions</h4>
<p>Either use Householder reflections or Givens rotations to compute QR.
Compute the complete orthogonal decomposition via two QR decompositions.
Compute the LU decomposition via Gauss/elimination matrices.</p>
<h4 id="multiple-rhs">Multiple RHS</h4>
<p>Suppose that we have some sequence of equations <span
class="math inline">Ax_i = b_i</span>. In this case, we form <span
class="math inline">B = \begin{matrix}b_1 &amp; b_2 &amp; \dots &amp;
b_n\end{matrix}</span> and solve <span class="math inline">AX =
B</span>. Everything from before carries through to <span
class="math inline">X = A^+B</span>, where <span
class="math inline">A^+</span> is the pseudoinverse.</p>
<h4 id="low-rank-updates">Low Rank Updates</h4>
<p>If you have a low-rank error, e.g. you solved <span
class="math inline">Ax = b</span> but in fact needed <span
class="math inline">\hat A = A + uv^T</span> instead, then you can solve
the new system efficiently by using the Sherman-Morrison formula, or in
general the Woodbury formula for higher rank corrections.</p>
</body>
</html>

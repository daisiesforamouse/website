<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Mathematical Computation I: Matrix Computation Course</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="wiki.css" />
  <link rel="stylesheet" href="/wiki.css" />
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Mathematical Computation I: Matrix Computation
Course</h1>
<h2 class="subtitle">UChicago STAT 30900, Autumn 2023</h2>
</header>
<hr>
<nav id="TOC" role="doc-toc">
<ul class="incremental">
<li><a href="#linear-algebra-review"
id="toc-linear-algebra-review">Linear Algebra Review</a></li>
<li><a href="#eigenvalues-and-eigenvectors"
id="toc-eigenvalues-and-eigenvectors">Eigenvalues and
Eigenvectors</a></li>
<li><a href="#singular-value-decomposition"
id="toc-singular-value-decomposition">Singular Value
Decomposition</a></li>
</ul>
</nav>
<hr>
<h3 id="linear-algebra-review">Linear Algebra Review</h3>
<hr />
<p>By convention, vectors are column vectors.</p>
<p>Set <span class="math inline">V</span> a vector space (almost always
real or complex).</p>
<p><em>Def</em>: <span class="math inline">\| \cdot \| : V \to \mathbb
R</span> is a <strong>norm</strong> if it satisfies the following
properties:</p>
<ul class="incremental">
<li><span class="math inline">\|v\| \geq 0</span> for any <span
class="math inline">v \in V</span>,</li>
<li><span class="math inline">\|v\| = 0</span> iff <span
class="math inline">v = 0</span>,</li>
<li><span class="math inline">\|\alpha v\| = |\alpha| \|v\|</span> for
<span class="math inline">\alpha \in \mathbb R</span> and <span
class="math inline">v \in V</span>,</li>
<li><span class="math inline">\|v + w\| \leq \|v\| + \|w\|</span> for
any <span class="math inline">v, w \in V</span>.</li>
</ul>
<p>Now, since this is a computational class, we only care about specific
norms, almost all of which we can quickly qrite down.</p>
<h4 id="vector-norms">Vector Norms</h4>
<p>Set <span class="math inline">V = \mathbb R^n</span> or <span
class="math inline">\mathbb C^n</span> equivalently.</p>
<p><em>Def</em>: The <strong>Minkowski</strong> or <span
class="math inline">p</span><strong>-norm</strong> is given by <span
class="math display">
  \| x \|_p = \left(\sum_{i=1} x_i^p\right)^{1/p}
</span> and we call the <span class="math inline">2</span>-norm the
<strong>Euclidean norm</strong> and the <span
class="math inline">1</span>-norm the <strong>Manhattan
norm</strong>.</p>
<p><em>Def</em>: The <span
class="math inline">\infty</span><strong>-norm</strong> is the limit of
<span class="math inline">p</span>-norms as <span class="math inline">p
\to \infty</span>, and is given by <span class="math display">
  \| x \|_\infty = \max_{i = 1, \dots, n} |x_i| = \lim_{p \to \infty} \|
x \|_p.
</span></p>
<p><em>Def</em>: For a weight vector <span
class="math inline">\underline w = \begin{bmatrix}w_1, \dots,
w_n\end{bmatrix}^T \in \mathbb R^n</span>, with each <span
class="math inline">w_i &gt; 0</span>, we have that the
<strong>weighted</strong> <span
class="math inline">p</span><strong>-norm</strong> is <span
class="math display">
  \| v \|_{\underline w, p} = \left(\sum_{i=1} w_i x_i^p\right)^{1/p}.
</span></p>
<p><em>Def</em>: In general, for any positive definite matrix <span
class="math inline">A</span> (that is, <span class="math inline">x^TAx
&gt; 0</span> for all <span class="math inline">x \neq 0</span>), we may
consider the <strong>Mahalanobis norm</strong> <span
class="math display">
  \| v \|_{A} = \left(x^T A x\right)^{1/2}.
</span></p>
<p>As convention, the “default” norm when a subscript is omitted is the
Euclidean norm.</p>
<h4 id="matrix-norms">Matrix Norms</h4>
<p>Set <span class="math inline">V = \mathbb R^{m \times n}</span> or
<span class="math inline">\mathbb C^{m \times n}</span>
equivalently.</p>
<p><em>Def</em>: The <strong>Hölder</strong> <span
class="math inline">p</span><strong>-norms</strong> are given by <span
class="math display">
  \| X \|_{H, p} = \left(\sum_{i=1}^m\sum_{j=1}^m |x_{ij}|^p
\right)^{1/p},
</span> and the Hölder <span class="math inline">2</span>-norm is called
the Frobenius norm, which is also defined on infinite dimensional vector
spaces as <span class="math display">
  \| X \|_F = \left(\mathop{\mathrm{Tr}}(XX^*)\right)^{1/2}
</span> where <span class="math inline">^*</span> is the conjugate
transpose.</p>
<p><em>Def</em>: As before, we can take <span class="math inline">p \to
\infty</span> to get the <strong>Hölder</strong> <span
class="math inline">\infty</span><strong>-norm</strong> given by <span
class="math display">
  \| X\|_{H, \infty} = \max_{\substack{i = 1, \dots n \\ j = 1, \dots,
n}} |x_{ij}|.
</span></p>
<p><em>Def</em>: We can also define norms on matrices by viewing them as
linear maps <span class="math inline">A: \mathbb R^n \to \mathbb
R^m</span>; in particular, if we have some norm <span
class="math inline">\| \cdot \|_a</span> on <span
class="math inline">\mathbb R^n</span> and some norm <span
class="math inline">\| \cdot \|_b</span> on <span
class="math inline">\mathbb R^m</span>, we may define the
<strong>operator norm</strong> (or <strong>induced norm</strong>) <span
class="math display">
  \| A \|_{a, b} = \max_{x \neq 0} \frac{\| A x \|_b}{\| x \|_a}.
</span> In particular, if the norms on the domain and codomain are just
<span class="math inline">p</span>-norms, we write <span
class="math display">
  \| A \|_{p} = \max_{x \neq 0}\frac{\| A x\|_p}{\| x \|_p}
</span> and call it the <span
class="math inline">p</span><strong>-norm</strong> of <span
class="math inline">A</span>. In particular, we call the <span
class="math inline">2</span>-norm the spectral norm. Further, the <span
class="math inline">1</span>-norm and <span
class="math inline">\infty</span>-norm are just <span
class="math display">
  \| A \|_1 = \max_{j = 1, \dots, n} \left(\sum_{i=1}^m |a_{ij}|
\right),
</span> which is the max column sum, and <span class="math display">
  \| A \|_\infty = \max_{i = 1, \dots, m} \left(\sum_{j=1}^n |a_{ij}|
\right),
</span> which is the max row sum; both facts are easy to check.</p>
<p>In general, for <span class="math inline">p \notin \{1, 2,
\infty\}</span>, computing <span class="math inline">\| A \|_p</span> is
NP-hard, and if we consider <span class="math inline">\| A
\|_{p,q}</span> then <span class="math inline">\|A\|_{\infty, 1}</span>
is hard and <span class="math inline">\|A\|_{1, \infty}</span> is
easy.</p>
<h4 id="properties-of-norms">Properties of Norms</h4>
<p>We may also want to consider some other desirable properties on our
norms.</p>
<ul class="incremental">
<li>For example, we might want <strong>submultiplicativity</strong>:
<span class="math display">
\| A B\| \leq \|A \| \| B \|.
</span> The Frobenius norm is submultiplicative.</li>
<li>Take also <strong>consistency</strong>: <span class="math display">
\| Ax \|_b \leq \| A \|_{a,b} \|x\|_a.
</span> This is true for <span class="math inline">p</span>-norms, but
not in general.</li>
</ul>
<p>Some properties always hold.</p>
<p><strong>Prop</strong>: Every norm is Lipschitz.</p>
<p><em>Proof</em>: Let our norm be <span class="math inline">\| \cdot \|
: V \to \mathbb{R}</span>. The triangle inequality immediately implies
<span class="math display">
  | \| u \| - \| v \| | \leq \| u - v \|.
</span></p>
<p><strong>Theorem (Equivalence of Norms)</strong>: <a
href="https://kconrad.math.uconn.edu/blurbs/gradnumthy/equivnorms.pdf">Link</a>
Set <span class="math inline">V</span> a finite-dimensional vector
space. Then every pair of norms <span class="math inline">\| \cdot \|_a,
\| \cdot \|_b</span> are equivalent to each other, e.g. there are
constants <span class="math inline">c_1, c_2</span> such that for any
<span class="math inline">v \in V</span>, <span class="math display">
  c_1\| v \|_b \leq \| v \|_a \leq c_2 \| v \|_b.
</span></p>
<p><em>Proof</em>: Induct on the dimension of <span
class="math inline">V</span> and see that every norm is equivalent to
the infinity norm.</p>
<p><em>Def</em>: We say that a sequence <span class="math inline">\{ x_k
\}_{k=1}^\infty</span> of vectors <strong>converges</strong> to <span
class="math inline">x</span> if <span class="math display">
  \lim_{k \to \infty} \| x_k - x \| = 0.
</span></p>
<p>Then, the above clearly shows that convergence in one norm implies
convergence in every norm.</p>
<h4 id="inner-outer-matrix-products">Inner, Outer, Matrix Products</h4>
<p><em>Def</em>: Set <span class="math inline">V</span> a <span
class="math inline">K</span>-vector space (where <span
class="math inline">K = \mathbb{R}, \mathbb{C}</span>). An <strong>inner
product</strong> is a binary operation <span class="math inline">\langle
\cdot, \cdot \rangle: V \times V \to \mathbb{R}</span> which satisfies
that</p>
<ul class="incremental">
<li><span class="math inline">\left\langle v, v \right\rangle \geq
0</span> for all <span class="math inline">v \in V</span>,</li>
<li><span class="math inline">\left\langle v, v,\right\rangle = 0</span>
if and only if <span class="math inline">v = 0</span>,</li>
<li><span class="math inline">\left\langle u, v \right\rangle =
\overline{ \left\langle v, u\right\rangle }</span> for all <span
class="math inline">u, v \in V</span>,</li>
<li><span class="math inline">\left\langle \alpha_1 u_1 + \alpha_2 u_2,
v\right\rangle = \alpha \left\langle u_1, v \right\rangle + \alpha_2
\left\langle u_2, v\right\rangle</span> for all <span
class="math inline">u_1, u_2, v \in V, \alpha_1, \alpha_2 \in
K</span>.</li>
</ul>
<p><strong>Prop</strong>: For an inner product <span
class="math inline">\left\langle \cdot, \cdot \right\rangle</span>,
<span class="math display">
  \| v \| = \sqrt{ \left\langle v, v \right\rangle }
</span> is a norm. Furthermore, an arbitrary norm <span
class="math inline">\| \|</span> is induced by an inner product if and
only if it satisfies the parallelogram law <span class="math display">
  \|u + v\|^2 + \|u - v\|^2 = 2 \|u\| ^ 2 + 2 \|v\| ^2.
</span></p>
<p><strong>Theorem (Cauchy-Schwarz)</strong>: <a
href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality">Link</a>
Let <span class="math inline">\|\cdot\|</span> be induced by <span
class="math inline">\left\langle \cdot, \cdot \right\rangle</span>. Then
<span class="math display">
  \sqrt{\left\langle u, v \right\rangle} \leq \|u\| \|v\|.
</span></p>
<p><em>Def</em>: The <strong>standard Euclidean inner product</strong>
on <span class="math inline">\mathbb{C}^n</span> is <span
class="math display">
  \left\langle x, y\right\rangle = x^*y.
</span></p>
<p><em>Def</em>: The <strong>Frobenius</strong> norm on <span
class="math inline">\mathbb{C}^{m \times n}</span> is <span
class="math display">
  \left\langle X, Y\right\rangle = \sum_{i=1}^m \sum_{j=1}^n x_{ij}
y_{ij} = \mathop{\mathrm{Tr}}(X^*Y).
</span></p>
<p><strong>Theorem (Hölder Inequality)</strong>: <a
href="https://en.wikipedia.org/wiki/H%C3%B6lder%27s_inequality">Link</a>
For <span class="math inline">x, y \in \mathbb{C}^n</span> and <span
class="math inline">p^{-1} + q^{-1} = 1</span>, we have <span
class="math display">
  |x^*y| \leq \|x\|_p \|y\|_q.
</span></p>
<p><strong>Theorem (Bessel Inequality)</strong>: <a
href="https://en.wikipedia.org/wiki/Bessel%27s_inequality">Link</a> For
<span class="math inline">x \in \mathbb{C}^n</span> and an orthonormal
basis <span class="math inline">e_1, \dots, e_n</span>, we have <span
class="math display">
  \sum_{k=1}^n \left| \left\langle x, e_k \right\rangle^2\right|  \leq
\|x\|_2.
</span></p>
<p><em>Def</em>: The <strong>outer product</strong> is a binary operator
<span class="math inline">\mathbb{C}^m \times \mathbb{C}^n \to
\mathbb{C}^{m \times n}</span> taking <span class="math display">
  (x, y) \mapsto xy^*.
</span></p>
<p><strong>Prop</strong>: <span class="math inline">A \in \mathbb{R}^{m
\times n}</span> is an outer product iff and only if it has rank 1.</p>
<p><em>Def</em>: The matrix product is a binary operator <span
class="math inline">\mathbb{C}^{m \times n} \times \mathbb{C}^{n \times
p} \to \mathbb{C}^{M \times p}</span>. Set <span class="math inline">A =
\left[\begin{matrix}\alpha_1 &amp; \alpha_2 &amp; \cdots &amp;
\alpha_m\end{matrix}\right]^T</span> and <span class="math inline">B =
\left[\begin{matrix}\beta_1 &amp; \beta_2 &amp; \cdots &amp;
\beta_p\end{matrix}\right]</span>. Then, <span class="math display">
  AB = \left[\begin{matrix}
    \alpha_1^T \beta_1 &amp; \cdots &amp; \alpha_1^T \beta_n \\
    \alpha_2^T \beta_1 &amp; \cdots &amp; \alpha_2^T \beta_n \\
    \vdots &amp; \ddots &amp; \vdots \\
    \alpha_m^T \beta_1 &amp; \cdots &amp; \alpha_m^T \beta_n
  \end{matrix}\right]
  = \left[\begin{matrix}A\beta_1 &amp; A\beta_2 &amp; \cdots &amp;
A\beta_n\end{matrix}\right]
  = \left[\begin{matrix}\alpha_1^TB \\ \alpha_2^TB \\ \vdots \\
\alpha_m^TB\end{matrix}\right].
</span> Alternatively, it is uniquely characterized as the matrix
representing the composition of <span class="math inline">A</span> and
<span class="math inline">B</span> as operators.</p>
<p><strong>Prop</strong>: Let <span class="math inline">D =
\mathop{\mathrm{diag}}(d_1, \dots, d_n)</span>, the diagonal matrix with
entries <span class="math inline">d_1, \dots, d_n</span>; then <span
class="math display">
  AD = \left[\begin{matrix}d_1\alpha_1, \dots, d_n
\alpha_n\end{matrix}\right].
</span> Simiar for the other direction of multiplication.</p>
<h3 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h3>
<hr />
<p><em>Def</em>: For a complex matrix <span
class="math inline">A</span>, an <strong>eigenvalue</strong> <span
class="math inline">\lambda \in \mathbb{C}</span> and
<strong>eigenvector</strong> <span class="math inline">v \neq 0</span>
satisfy <span class="math display">
  Av = \lambda v.
</span></p>
<p><em>Def</em>: Furthermore, an <strong>eigenspace</strong> is the span
of all eigenvectors correspnding to a single eigenvalue, the
<strong>spectrum</strong> of <span class="math inline">A</span>, <span
class="math inline">\mathop{\mathrm{Spec}}(A)</span> is the set of all
eigenvalues of <span class="math inline">A</span>, and the
<strong>spectral radius</strong> is <span class="math inline">\rho(A) =
\max_{\lambda \in \mathop{\mathrm{Spec}}(A)} |\lambda| =
|\lambda_{\text{max}}|</span>. Sometimes we will call this the top
eigenvector/eigenvalue.</p>
<p>As a convention, usually we implcitly order eigenvalues, e.g. <span
class="math inline">|\lambda_1| \geq |\lambda_2| \geq \cdots \geq
\lambda_n</span>.</p>
<p><em>Def</em>: More generally, if <span class="math inline">v</span>
is a eigenvector of <span class="math inline">A^T</span>, we say that
<span class="math inline">v</span> is a <strong>left
eigenvector</strong> of <span class="math inline">A</span> (and thus
usual eigenvectors are <strong>right eigenvectors</strong>).</p>
<p>There are a few warnings about these things:</p>
<ul class="incremental">
<li>real matrices may have complex eigenvectors;</li>
<li>we normally normalize eigenvectors to have unit length;</li>
<li>left and right eigenvectors are usually not the same.</li>
</ul>
<p><em>Def</em>: A square matrix <span class="math inline">A \in
\mathbb{C}^{n \times n}</span> is diagonalizable if it is simiarly to a
diagonal matrix.</p>
<p><strong>Prop</strong>: <span class="math inline">A</span> is
diagonalizable if and only if it has <span class="math inline">n</span>
linearly independent eigenvectors.</p>
<p><em>Proof</em>: The eigenvectors form a basis by intertibility;
change basis from the eigenvectors, scale by eigenvalues, and change
basis back. In particular, let <span class="math inline">A \in
\mathbb{C}^{n \times n}</span> have eigenvalues <span
class="math inline">\lambda_1, \dots, \lambda_n</span>, with
corresponding eigenvectors <span class="math inline">v_1, \dots,
v_n</span>; set <span class="math inline">\Lambda =
\mathop{\mathrm{diag}}(\lambda_1, \dots, \lambda_n)</span> and <span
class="math inline">V = \left[\begin{matrix}v_1, \dots,
v_n\end{matrix}\right]</span>. Then <span class="math display">
  A = V \Lambda V^{-1}.
</span></p>
<p><em>Def</em>: The above <span class="math inline">A = X \Lambda
X^{-1}</span> is called the <strong>eigenvalue decomposition</strong>,
or <strong>EVD</strong>.</p>
<p><strong>Prop</strong>: The diagonal entries in a triangular matrix
are just the eigenvalues.</p>
<p><em>Def</em>: A matrix <span class="math inline">A \in \mathbb{C}^{n
\times n}</span> is <strong>normal</strong> if it commutes with its
adjoint <span class="math inline">A^*</span> (the conjugate transpose),
and <strong>unitary</strong> if <span class="math inline">AA^* = A^*A =
I</span>.</p>
<p><strong>Prop</strong>: <span class="math inline">A</span> is unitary
if and only if it’s columns (or rows) are orthonormal.</p>
<p><strong>Theorem</strong>: <span class="math inline">A \in
\mathbb{C}^{n \times n}</span> is normal if and only if it is unitarily
diagonalizable, e.g. if <span class="math inline">A = V\Lambda
V^*</span> with <span class="math inline">V</span> unitary.</p>
<p><em>Def</em>: <span class="math inline">A \in \mathbb{C}^{n \times
n}</span> is <strong>Hermitian</strong> if <span class="math inline">A^*
= A</span>.</p>
<p><strong>Theorem (Spectral Theorem)</strong>: <a
href="https://en.wikipedia.org/wiki/Spectral_theorem">Link</a> <span
class="math inline">A \in \mathbb{C}^{n \times n}</span> is Hermetian if
and only if <span class="math inline">A</span> is unitarily
diagonalizable with all real eigenvalues.</p>
<p><strong>Corollary</strong>: <span class="math inline">A \in
\mathbb{R}^{n \times n}</span> is symmetric if and only if it is
orthgonally diagonalizable with all eigenvalues real.</p>
<h4 id="jordan-canonical-form">Jordan Canonical Form</h4>
<p><em>Def</em>: <a
href="https://en.wikipedia.org/wiki/Jordan_normal_form">Link</a> Any
matrix can be written in <strong>Jordan canonical form</strong>, e.g.
<span class="math display">
  A = XJX^{-1}
</span> where <span class="math inline">J</span> is nonzero only on the
diagonal and the superdiagonal, which has values only in <span
class="math inline">\{0, 1\}</span>, such as <span class="math display">
  J = \begin{bmatrix}
    \lambda_1 &amp; 1         &amp;           &amp;   &amp; &amp; &amp;
\\
              &amp; \lambda_1 &amp; 1         &amp;   &amp; &amp; &amp;
\\
              &amp;           &amp; \lambda_1 &amp; 0 &amp; &amp; &amp;
\\
              &amp;           &amp; &amp; \lambda_2 &amp; 0 &amp; &amp;
\\
              &amp;           &amp;         &amp;  &amp; \lambda_3 &amp;
1 &amp; \\
              &amp;           &amp;         &amp;  &amp; &amp;
\lambda_3  &amp; \\
  \end{bmatrix}
</span> for example.</p>
<p>The way it’s written above is not by coincidence: you can permute
everything so that <span class="math inline">J</span> is composed of
Jordan blocks, which have the same entry all the way down the diagonal
and have a superdiagonal of only ones, e.g. <span class="math display">
  J = \begin{bmatrix}
    J_1 &amp; &amp; &amp; \\
    &amp; J_2 &amp; &amp; \\
    &amp; &amp; \ddots &amp; \\
    &amp; &amp; &amp; J_k \\
  \end{bmatrix}
</span> where each <span class="math inline">J_i = \lambda I + N</span>,
where <span class="math inline">N</span> has all zero entries except on
the superdiagonal, on which it is always one.</p>
<p>Unfortunately, the JCF is pretty useless in application.</p>
<p><strong>Theorem (Golub-Wilkinson)</strong>: The Jordan canonical form
cannot be computed in finite precision.</p>
<h4 id="spectral-radius">Spectral Radius</h4>
<p>Let’s return to the spectral radius, <span class="math display">
  \rho(A) = \max_{\lambda \in \mathop{\mathrm{Spec}}(A)} |\lambda|.
</span></p>
<p>Any nilpotent matrix shows that <span class="math inline">\rho</span>
is not a norm, but the following is true.</p>
<p><strong>Prop</strong>: If <span class="math inline">\| \cdot \|:
\mathbb{C}^{m \times m} \to \mathbb{R}</span> is any consistent matrix
norm, then <span class="math inline">\rho(A) \leq \| A \|</span>.</p>
<p><em>Proof</em>: Look at the norm of the image of a top
eigenvector.</p>
<p><strong>Theorem</strong>: Given any <span class="math inline">A \in
\mathbb{C}^{n \times n}</span>, any positive <span
class="math inline">\epsilon</span>, there is a consistent norm (in
fact, an operator norm) <span class="math inline">\| \cdot \|:
\mathbb{C}^{n \times m} \to \mathbb{R}</span> such that <span
class="math display">
  \| A \| \leq \rho(A) + \epsilon.
</span></p>
<p><strong>Prop</strong>: Given any matrix norm <span
class="math inline">\| \cdot \|: \mathbb{C}^{n \times m} \to R</span>,
we have <span class="math display">
  \rho(A) = \lim_{k \to \infty} \| A^k \|^{1/k}.
</span></p>
<p><strong>Lemma</strong>: <span class="math inline">\lim_{k \to \infty}
A^k = 0</span> if and only if <span class="math inline">\rho(A) &lt;
1</span>.</p>
<p><em>Proof</em>: <span class="math inline">(\implies)</span> Set <span
class="math inline">\lambda</span> to be a top eigenvalue of <span
class="math inline">A</span>, and <span class="math inline">x</span> a
corresponding eigenvector. Then, <span class="math inline">A^k x =
\lambda^k x</span>. Send <span class="math inline">k \to \infty</span>
and conclude.</p>
<p><span class="math inline">(\impliedby)</span> By the above theorems,
there is some operator norm <span class="math inline">\| \cdot \|</span>
such that <span class="math inline">\|A\| \leq \rho(A) + \epsilon &lt;
1</span>. Send <span class="math inline">k \to \infty</span> and use the
fact that operator norms imply <span class="math inline">\| A^k \| \leq
\|A\|^k</span> and win.</p>
<h4 id="finding-eigenvalues">Finding Eigenvalues</h4>
<p><strong>Theorem (Gershgorin Circle Theorem)</strong>: <a
href="https://en.wikipedia.org/wiki/Gershgorin_circle_theorem">Wikipedia</a>
Let <span class="math inline">A \in \mathbb{C}^{n \times n}</span>, with
entries <span class="math inline">a_{ij}</span> and for <span
class="math inline">1 \leq i \leq n</span> set <span
class="math inline">r_i = \sum_{j\neq i} |a_{ij}|</span>. Then, every
eigenvalue of <span class="math inline">A</span> lies within the union
of the Gershgorin discs <span class="math display">
  G_i = \left\{ z \mid z \in \mathbb{C}, |z - a_{ii}| \leq \sum_{j \neq
i} |a_{ij}| \right\}
</span> and the number of eigenvalues in each connected component is
equal to the number of Gershgorin disks that constitute that
component.</p>
<p><em>Proof</em>: If <span class="math inline">A \in \mathbb{C}^{n
\times n}</span> is strictly diagonally dominant, e.g. <span
class="math display">
  |a_{ii}| &gt; \sum_{j \neq i}|a_{ij}|
</span> for all <span class="math inline">1 \leq i \leq n</span>, then
<span class="math inline">A</span> is invertible. To see this, take any
<span class="math inline">x \in \ker(A)</span> so that <span
class="math inline">\sum_{j=1}^n a_{ij}x_i = 0</span>. But look at the
index <span class="math inline">k</span> that witnesses <span
class="math inline">\| x \|_\infty</span>: <span class="math display">
  -a_{kk}x_k = \sum_{j \neq k} a_{kj}x_j \implies |a_{kk}||x_k| \leq
\sum_{j \neq k} |a_{kj}||x_k|
</span> so <span class="math inline">x = 0</span> and <span
class="math inline">\ker(A)</span> is trivial.</p>
<p>We proceed to prove that any <span class="math inline">z \notin
\bigcup_{i=1}^n G_i</span> cannot be an eigenvalue by showing that <span
class="math inline">A - zI</span> is invertible. But this is clear,
since <span class="math inline">A - zI</span> in that case is strictly
diagonally dominant.</p>
<p><em>Def</em>: <a
href="https://en.wikipedia.org/wiki/Schur_decomposition">Wikipedia</a>
The <strong>Schur decomposition</strong> of a matrix <span
class="math inline">A</span> is <span class="math inline">A =
QUQ^*</span> such that <span class="math inline">Q</span> is unitary and
<span class="math inline">U</span>, the <strong>Schur form</strong> of
<span class="math inline">A</span> is upper triangular.</p>
<p>Note that you can read off the eigenvalues of <span
class="math inline">A</span> from the diagonal of the Schur form, which
is numerically stable to compute.</p>
<p>As an aside, how one numerically finds the roots of a polynomial
<span class="math inline">p(x) = \sum_{k=0}^m c_kx^k</span> is by
forming the companion matrix <span class="math display">
  A = \begin{bmatrix}
    0 &amp; \cdots &amp; 0 &amp; -c_0 \\
    1 &amp; \ddots &amp;0 &amp; -c_1 \\
    \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
    0 &amp; \cdots &amp; 1 &amp; -c_{d-1}
  \end{bmatrix}.
</span></p>
<h3 id="singular-value-decomposition">Singular Value Decomposition</h3>
<hr />
<p><em>Def</em>: <a
href="https://en.wikipedia.org/wiki/Singular_value_decomposition">Link</a>
The <strong>SVD decomposition</strong> of a real (resp. complex) matrix
<span class="math inline">A \in \mathbb{C}^{m \times n}</span>) is <span
class="math display">
  A = U \Sigma V^*
</span> where <span class="math inline">U, V</span> are orthogonal
(resp. unitary) and <span class="math inline">\Sigma</span> is diagonal
(on the shorter diagonal) with nonnegative entries; that is, if <span
class="math inline">m &gt; n</span>, then <span class="math display">
  \Sigma = \left[\begin{matrix}\mathop{\mathrm{diag}}(\sigma_1, \dots,
\sigma_n) \\ \mathbf 0\end{matrix}\right] \in \mathbb{R}^{m \times
n}_{\geq 0}
</span> and if <span class="math inline">m &lt; n</span>, <span
class="math display">
  \Sigma = \left[\begin{matrix}\mathop{\mathrm{diag}}(\sigma_1, \dots,
\sigma_n) &amp; \mathbf 0\end{matrix}\right] \in \mathbb{R}^{m \times
n}_{\geq 0}
</span></p>
<p><em>Def</em>: We may arrange <span class="math inline">\sigma_{1}
\geq \sigma_2 \geq \dots \geq \sigma_{\min(m, n)} \geq 0</span>; we then
call top few singular values the principal singular values. The columns
of <span class="math inline">U</span> are left singular vectors, and the
columns of <span class="math inline">V</span> are right singular
vectors.</p>
<p><strong>Prop</strong>: The largest <span class="math inline">r</span>
for which <span class="math inline">r &gt; 0</span> is the rank of <span
class="math inline">A</span>.</p>
<p><em>Def</em>: We sometimes use the <strong>reduced SVD</strong>,
which is the same as the normal SVD but we drop the extra rows of <span
class="math inline">\Sigma</span> to force it to be a square diagonal
matrix in <span class="math inline">\mathbb{C}^{\min\{m, n\} \times
\min\{m, n\}}</span>. Then, <span class="math display">
  A = \sum_{k=1}^r \sigma_k u_r v_r^*
</span> where <span class="math inline">u_r, v_r</span> are the columns
of <span class="math inline">U, V</span> in the reduced SVD and <span
class="math inline">r = \operatorname{rank}(A)</span>, or the number of
nonzero singular values.</p>
<p><em>Def</em>: In a similar vein, the <strong>condensed SVD</strong>
is attained by removing all of the nonzero rows/columns of <span
class="math inline">\Sigma</span>, so <span class="math inline">\Sigma
\in \mathbb{C}^{r \times r}</span>.</p>
<p><strong>Theorem</strong>: Given any <span class="math inline">A \in
\mathbb{C}^{m \times n}</span>, there are <span class="math inline">U
\in \mathbb{C}^{m \times m}</span>, <span class="math inline">V \in
\mathbb{C}^{n \times n}</span>, and <span class="math display">
  \Sigma = \mathop{\mathrm{diag}}(\sigma_1, \dots, \sigma_r, 0, \dots,
0) \in \mathbb{R}_{\geq 0}^{m \times n}
</span> such that <span class="math inline">U, V</span> are unitary and
<span class="math inline">A = U \Sigma V^*</span>.</p>
<p><em>Proof</em>: Form the matrix <span class="math display">
  W = \left[\begin{matrix}0 &amp; A \\ A^* &amp; 0\end{matrix}\right]
</span> which is Hermitian. The spectral theorem gives an EVD, and if
you look carefully you get the compact SVD: <span class="math display">
  W = \left[\begin{matrix} U_r &amp; U_r \\ V_r &amp; -V_r
\end{matrix}\right] \left[\begin{matrix}
  \sigma_1 &amp; &amp; &amp; &amp; &amp;  &amp;\\
   &amp; \sigma_2 &amp; &amp; &amp; &amp; &amp;\\
   &amp; &amp; \ddots &amp; &amp; &amp; &amp;\\
   &amp; &amp;  &amp; \sigma_r &amp; &amp; &amp;\\
   &amp; &amp;  &amp; &amp; -\sigma_1 &amp; &amp; \\
   &amp; &amp;  &amp; &amp; &amp; \ddots &amp; \\
   &amp; &amp;  &amp; &amp; &amp; &amp; -\sigma_r\\ \end{matrix}\right]
  \left[\begin{matrix} U_r &amp; U_r \\ V_r &amp; -V_r
\end{matrix}\right]^*.
</span></p>
<p>Singular values have a nice property: we have for any singular value
<span class="math inline">\sigma</span> and left singular vector <span
class="math inline">u</span> and right singular value <span
class="math inline">v</span>, <span class="math inline">Av = \sigma
u</span> and <span class="math inline">A^*v = \sigma v</span>; thus
<span class="math inline">u, v</span> are eigenvalues of <span
class="math inline">AA^*</span> and <span
class="math inline">A^*A</span> respectively.</p>
<h4 id="applications">Applications</h4>
<p>We can read off many important quantities/spaces from the SVD. Let
<span class="math inline">A = U \Sigma V^*</span> be a SVD, with only
<span class="math inline">r</span> nonzero singular values.</p>
<ul class="incremental">
<li>The rank is the number of nonzero singular values.</li>
<li>The absolute value of the determinant is the product of the signular
values.</li>
<li>The two norm is the maximum singular value.</li>
<li>Set <span class="math inline">\sigma = (\sigma_1, \dots,
\sigma_n)</span>; then the Frobenius norm of <span
class="math inline">A</span> is <span class="math inline">\| \sigma
\|_2</span>, and we call the general case of this the <strong>Schatten
norm</strong>, e.g. <span class="math inline">\| A \|_{S, p} = \| \sigma
\|_p</span>; the case of <span class="math inline">p = 1</span> is also
called the <strong>nuclear norm</strong>.</li>
<li>The Ky Fan <span class="math inline">(p, k)</span> norm for an
integer <span class="math inline">1 \leq k \leq \infty</span> is <span
class="math display">
  \| A \|_{S, p, k} = \left( \sum_{i=1}^p \sigma_i^p \right)^{1/p}.
</span></li>
<li>The kernel of <span class="math inline">A = U \Sigma V^*</span> is
spanned by <span class="math inline">v_{r+1}, \dots, v_n</span>.</li>
<li>The image of <span class="math inline">A</span> is <span
class="math inline">u_1, \dots, u_r</span>.</li>
<li>The kernel of <span class="math inline">A^T</span> (this is the
cokernel) is spanned by <span class="math inline">v_{1, \dots,
v_r}</span>.</li>
<li>The image of <span class="math inline">A^T</span> is spanned by
<span class="math inline">u_{r+1}, \dots, u_m</span>.</li>
</ul>
<p>You can also solve fundamental problems. For example, if you consider
the linear system <span class="math inline">Ax = b</span>, it might not
have exactly one solution, so we can translate this into the least
squares system <span class="math inline">\min_{x \in \mathbb{R}^n} \| Ax
- b \|_2^2</span>; this still might not have a unique solution when
<span class="math inline">A</span> is singular, so we consider the
<strong>minimum norm least squares</strong> problem: <span
class="math inline">\min \{ \| x \|_2 \mid x \in
\mathop{\mathrm{argmin}}\| A_x - b \|_2 \}</span>.</p>
<p>SVD solves all of these simultaneously; set <span
class="math inline">A = U \Sigma V^T</span>; this means that we want
<span class="math inline">\Sigma V^T x = U^T b</span>; equivalently, we
just need to solve <span class="math inline">\Sigma y = c</span>, where
<span class="math inline">b = Uc, x = Vy</span>. But this is trivial
since <span class="math inline">\Sigma</span> is diagonal.</p>
<p>We could define the pseudoinverse of <span
class="math inline">A</span> to be <span class="math inline">A^+</span>
to be the matrix <span class="math inline">A^+</span> that sends any
<span class="math inline">b</span> to the minimum norm least squares
solution of <span class="math inline">Ax = b</span>.</p>
<p><strong>Prop</strong>: <span class="math inline">A^+</span> is just
<span class="math inline">V \Sigma^{-1} U^*</span>, where by <span
class="math inline">\Sigma^{-1}</span> is just gotten by flipping all
the nonzero diagonal elements.</p>
<p><em>Proof</em>: Clear.</p>
<p><em>Def</em>: In general, the <strong>pseudoinverse</strong> or
<strong>Moore-Penrose</strong> inverse of <span class="math inline">A
\in K^{m \times n}</span> is a matrix <span class="math inline">X \in
K^{n \times m}</span> that satisfies</p>
<ul class="incremental">
<li><span class="math inline">(AX)^{*} = AX</span>,</li>
<li><span class="math inline">(XA)^* = XA</span>,</li>
<li><span class="math inline">AXA = A</span>,</li>
<li><span class="math inline">XAX = X</span>.</li>
</ul>
<p><strong>Prop</strong>: The above gives a unique matrix and is in fact
the same as <span class="math inline">A^+</span> from earlier.</p>
<p><strong>Prop</strong>: There following statements are true.</p>
<ul class="incremental">
<li><span class="math inline">A^+A</span> and <span
class="math inline">AA^+</span> are not necessarily <span
class="math inline">I</span>.</li>
<li>If <span class="math inline">A</span> has full column rank <span
class="math inline">A^+ = (A^*A)^{-1}A</span>.</li>
<li>If <span class="math inline">A</span> has full row rank, <span
class="math inline">A^+ = A^*(AA^*)^{-1}</span>.</li>
</ul>
</body>
</html>

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Brownian Motion and Stochastic Calculus</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script defer=""
  src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css" />
  <link rel="stylesheet" href="wiki.css" />
  <link rel="stylesheet" href="/wiki.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Brownian Motion and Stochastic Calculus</h1>
<h2 class="subtitle">UChicago STAT 38510, Autumn 2023</h2>
</header>
<hr>
<nav id="TOC" role="doc-toc">
<ul class="incremental">
<li><a href="#brownian-motion" id="toc-brownian-motion">Brownian
Motion</a></li>
<li><a href="#brownian-motion-in-several-dimensions"
id="toc-brownian-motion-in-several-dimensions">Brownian Motion in
Several Dimensions</a></li>
<li><a href="#differential-equations"
id="toc-differential-equations">Differential Equations</a></li>
<li><a href="#stochastic-integration"
id="toc-stochastic-integration">Stochastic Integration</a></li>
<li><a href="#diffusions" id="toc-diffusions">Diffusions</a></li>
<li><a href="#integrals-against-continuous-martingales"
id="toc-integrals-against-continuous-martingales">Integrals Against
Continuous Martingales</a></li>
<li><a href="#conformal-invariance"
id="toc-conformal-invariance">Conformal Invariance</a></li>
<li><a href="#levy-processes" id="toc-levy-processes">Levy
Processes</a></li>
</ul>
</nav>
<hr>
<h3 id="brownian-motion">Brownian Motion</h3>
<hr />
<p>As a style preference, I’m going to drop all the arguments that are
from the probability space.</p>
<p>Fix a probability space <span class="math inline">(\Omega, \mathcal
F, P)</span>; we characterize the Brownian motion <span
class="math inline">\{B_t\}_{t \geq 0}</span> via the following
properties:</p>
<ul class="incremental">
<li><strong>Independent Increments</strong>: If <span
class="math inline">s &lt; t</span>, the random variable <span
class="math inline">B_t - B_s</span> is independent of <span
class="math inline">\sigma\{B_r: r \leq s\}</span></li>
<li><strong>Stationary Increments</strong>: If <span
class="math inline">s &lt; t</span>, then <span class="math inline">B_t
- B_s</span> has the same distribution as <span
class="math inline">B_{t-s} - B_0</span>.</li>
<li><strong>Continuity</strong>: The map <span class="math inline">t
\mapsto B_t</span> is almost surely continuous.</li>
</ul>
<p><strong>Theorem</strong>: If a process satisfies the above, then
there are <span class="math inline">\mu, \sigma^2</span> (respectively
called the drift and the variance parameter, and <span
class="math inline">\sigma</span> is named the volatility) such that
there exist <span class="math inline">B_t \sim N(\mu t, \sigma^2
t)</span>.</p>
<p><em>Def</em>: A stochastic process <span
class="math inline">\{B_t\}_{t \geq 0}</span> is called a (one
dimensional) <strong>Brownian motion</strong> (or Wiener process)
starting from the origin with drift <span class="math inline">\mu</span>
and variance parameter <span class="math inline">\sigma^2</span> if
<span class="math inline">B_t = 0</span> and the above three conditions
are satisfied, with the imposition that <span class="math display">
B_t - B_s \sim N(\mu (t-s), \sigma^2 (t-s)).
</span></p>
<p><strong>Prop</strong>: If <span class="math inline">B_t</span> is a
Brownian motion with <span class="math inline">\mu = 0, \sigma^2 =
1</span> (a so-called <strong>standard Brownian motion</strong>), then
<span class="math inline">Y_t = \sigma B_t + \mu t</span> is a Brownian
motion with parameters <span class="math inline">\mu,
\sigma^2</span>.</p>
<p><em>Proof</em>: Obvious.</p>
<h4 id="construction">Construction</h4>
<p>Pick a probability space <span class="math inline">(\Omega, \mathcal
F, P)</span> that is rich enough to support a countable collection of
independent standard normal variables. If you are particular, the unit
interval with Lesbegue measure is sufficient here.</p>
<p>The strategy is as follows: we define <span
class="math inline">B_t</span> for a countable dense set (in particular
the dyadic rationals) of times using our precession of standard normals.
then, we find some <span class="math inline">t \mapsto B_t</span> that
agrees on the dense set and is uniformly continuous and then extend by
continuity.</p>
<p>Set <span class="math inline">D_n = \left\{ \frac{k}{2^n}, k = 0, 1,
\dots, 2^n \right\}</span> and <span class="math inline">D =
\bigcup_{n=0}^\infty D_n</span>; index our standard normals by <span
class="math inline">\{N_{q}\}_{q \in D}</span>, and set <span
class="math inline">B_0 = 0, B_1 = N_1</span>, and <span
class="math inline">B_{1/2} = \frac{B_1 - B_0}{2} +
\frac{1}{2}N_{1/2}</span>. Just continue the same thing for every such
dyadic, such that <span class="math display">
\{B_{1/2^n} - B_0, B_{2/2^n} - B_{1/2^n}, \dots, B_1- B_{(2^n-1) / 2^n}
\}
</span> are all independent <span class="math inline">N\left(0,
2^{-n}\right)</span>.</p>
<p><strong>Theorem</strong>: Almost surely, <span class="math inline">t
\mapsto B_t</span>, <span class="math inline">t \in D</span> is
uniformly continuous.</p>
<p><em>Proof</em>: Set <span class="math inline">K_n = \sup \{ |B_s -
B_t| \mid s,t \in D, |s - t| \leq 2^{-n}\}</span>. We just need to show
that <span class="math inline">K_n \to 0</span> as <span
class="math inline">n \to \infty</span>. In fact, something even
stronger is true: for <span class="math inline">\alpha &lt;
\frac{1}{2}</span>, <span class="math inline">\lim_{n \to \infty}
2^{\alpha n}K_n = 0</span>. Morally speaking, just think that each
Brownian increment is about its standard deviation, which is <span
class="math inline">|t - s|^{1/2}</span>.</p>
<p>Technically, however, we proceed as follows: set <span
class="math display">
  Y_n = \max\{B_{1/2^n} - B_0, B_{2/2^n} - B_{1/2^n}, \dots, B_1-
B_{(2^n-1) / 2^n} \}
</span> and note that the union bound yields <span class="math display">
\begin{align*}
  P(Y_n \geq x) &amp;\leq \sum_{j=1}^{2^n} P(|B_{j/2^n} - B_{(j-1)/2^n}|
\geq x) \\
      &amp;= 2^nP(B_{1/2^n} \geq x) \\
      &amp;= 2^{n+1}P(B_1 \geq 2^{n/2} x).
\end{align*}
</span> If we choose <span class="math inline">x_n</span> such that
<span class="math inline">\sum_{n=1}^\infty 2^{n+1}P(B_1 \geq
2^{n/2}x_n) &lt; \infty</span>, then by Borel-Cantelli shows that for
sufficiently large <span class="math inline">n</span>, <span
class="math inline">Y_n \leq x_n</span> almost surely. Do any reasonable
bound you like on the tail of the normal distribution and take a
sufficiently large <span class="math inline">x</span> and call it a day.
In particular, if you choose the easiest bound <span
class="math inline">P(N \geq x) \leq Ce^{-x^2/2}</span>, you can
eventually get the bound:</p>
<p><strong>Prop</strong>: <span class="math display">
  \limsup_{n \to \infty} \frac{2^{n/2}}{\sqrt{n}}Y_n \leq \sqrt{2 \log
2}.
</span></p>
<p><em>Proof</em>: Look at the sum <span class="math display">
  \sum_{n=1}^\infty P\left(Y_n &gt; \sqrt{n} \cdot 2 ^{-n/2} \cdot
\sqrt{2 \log 2 (1 + \epsilon)}\right)
</span> and apply Borel-Cantelli. In particular, we have <span
class="math display">
\begin{align*}
  P(Y_n &gt; x_n) &amp;\leq \sum_{j=1^{2^n}} P(|B_{j/2^n} -
B_{(j-1)/2^n}| &gt; x_n) \\
  &amp;= 2^{n+1}P(B_{1/2^n} &gt; x_n) \\
  &amp;= 2^{n+1}P\left(B_1 &gt; \sqrt{n} \sqrt{2(\log 2 (1 +
\epsilon)}\right) \\
  &amp;\leq C 2^n e^{-\frac{\sqrt{2\log 2 n(1+\epsilon)^2}}{2}} \\
  &amp;\leq C e^{-n\epsilon}.
\end{align*}
</span></p>
<p><strong>Prop</strong>: Set <span class="math inline">K_n = \sup \{
|B_s - B_t| \mid s,t \in D, |s - t| \leq 2^{-n}\}</span>; then, there is
<span class="math inline">C</span> such that with almost surely, <span
class="math display">
  \limsup_{n \to \infty} \frac{2^{n/2}}{\sqrt{n}}K_n \leq C.
</span></p>
<p><em>Proof</em>: It’s easy to see that <span class="math inline">K_n
\leq 2 \sum_{j=n+1}^\infty Y_j</span> (this is just the triangle
inequality). Then for sufficiently large <span
class="math inline">n</span>, we get <span class="math display">
  K_n \leq 2 \cdot 2 \sum_{j=n+1}^\infty 2^{-j/2}\sqrt{j}
</span> with full probability, and so <span class="math display">
  \sup_{\substack{s, t \in D \\ s &lt; t}} \frac{|B_t -
B_s|}{\sqrt{(t-s)|\log((t-s)^{-1})|}} &lt; \infty.
</span></p>
<p>Now we may set <span class="math inline">B_t</span> for <span
class="math inline">t \in [0, 1]</span> by <span class="math inline">B_t
= \lim_{\substack{s \to t \\ s \in D}}B_s</span>, and check that this is
in fact a genuine Brownian motion, which is not bad; and of course this
construction can extend to <span class="math inline">[0, \infty)</span>
easily as well.</p>
<h4 id="properties-of-brownian-motion">Properties of Brownian
Motion</h4>
<p><em>Def</em>: A function <span class="math inline">f: [0, 1] -&gt;
\mathbb R</span> is called <strong>Hölder continuous</strong> of order
<span class="math inline">\beta \geq 0</span> if there is some <span
class="math inline">C &lt; \infty</span> such that for all <span
class="math inline">s, t</span>, <span class="math inline">|f(t) - f(s)|
\leq C|t-s|^\beta</span>. Futher, <span class="math inline">f</span> is
<strong>weakly Hölder continuous</strong> of order <span
class="math inline">\beta</span> if it is Hölder continuous of order
<span class="math inline">\alpha</span> for all <span
class="math inline">\alpha &lt; \beta</span>. In both cases, we will say
Hölder-<span class="math inline">\beta</span> continuous for short.</p>
<p><strong>Prop</strong>: Brownian motion paths are weakly Hölder-<span
class="math inline">\frac{1}{2}</span> continuous.</p>
<p><em>Proof</em>: Omitted.</p>
<p><strong>Theorem</strong>: The function <span class="math inline">t
\mapsto B_t</span> is nowhere differentiable almost surely.</p>
<p><em>Proof</em>: Assume <span class="math inline">|f&#39;(t)| &lt;
K</span>; there exists <span class="math inline">\delta &gt; 0</span>
such that if <span class="math inline">|s - t| \leq \delta</span>, <span
class="math inline">|f(t) - f(s)| \leq 2K|s-t|</span>; in particular
there is an <span class="math inline">N</span> such that for all <span
class="math inline">n &gt; N</span>, <span class="math inline">|s - t|
\leq n^{-1}, |r - t| \leq n^{-1}</span>, <span class="math inline">|f(s)
- f(r)| \leq 4Kn^{-1}</span>. Then, set <span class="math display">
  Z_{k, n} = \max \left\{ |B_{k/n} - B_{(k-1)/n}|, |B_{(k+1)/n} -
B_{k/n}|, |B_{(k+2)/n} - B_{(k+1)/n}|\right\}
</span> and <span class="math display">
  Z_n = \min \{ Z_{k, n} \mid k = 1, \dots, n \}.
</span></p>
<p>If <span class="math inline">B</span> is differentiable, then there
is some <span class="math inline">M</span> such that <span
class="math inline">Z_n \leq Mn^{-1}</span> for all <span
class="math inline">n</span>. Now set <span
class="math inline">E_M</span> to be the event that <span
class="math inline">Z_n \leq Mn^{-1}</span> for all sufficiently large;
our theorem reduces to showing that <span class="math inline">P(E_M)  =
0</span> for all <span class="math inline">M</span>. In fact, we will
show that <span class="math display">
  \lim_{n \to \infty} P(Z_n \leq Mn^{-1}) = 0.
</span></p>
<p>Consider the union bound <span class="math display">
\begin{align*}
  P(Z_n \leq Mn^{-1}) &amp;\leq \sum_{j=1}^{n}P(Z(n,k) \leq Mn^{-1}) \\
  &amp;\leq nP\left( \max \left\{ |B_{1/n}|, |B_{2/n} - B_{1/n}|,
|B_{3/n} - B_{2/n}|\right\}
\right) \\
&amp;\leq nP(|B_{1/n}| \leq Mn^{-1})^3 \\
&amp;\leq nP(|B_1| \leq Mn^{-1/2})^3
\end{align*}
</span> and just do literally the stupidest estimate you can, e.g. just
look at the density and say that the probability is bounded by <span
class="math inline">2CMn^{-1/2}</span>, so that the above is sent to
zero as <span class="math inline">n \to \infty</span>.</p>
<h4 id="filtrations">Filtrations</h4>
<p><em>Def</em>: A filtration <span class="math inline">\{ \mathcal F
\}_{t \geq 0}</span> is an incerasing collection of sub <span
class="math inline">\sigma</span>-algebras. Further, we put <span
class="math display">
  \mathcal F_{\infty} = \bigcup_{t \geq 0} \mathcal F_t.
</span></p>
<p><em>Def</em>: A stochastic process <span class="math inline">\{X_t
\}_{t \geq 0}</span> is adapted to <span class="math inline">\{ \mathcal
F_t \}_{t \geq 0}</span> if for each <span class="math inline">t</span>,
<span class="math inline">X_t</span> is <span
class="math inline">\mathcal F_t</span>-measurable.</p>
<p><em>Def</em>: A process <span class="math inline">\{ B_t \}_{t \geq
0}</span> is a standard Brownian motion start at 0 w.r.t. <span
class="math inline">\{ \mathcal F_t \}_{t \geq 0}</span> if</p>
<ul class="incremental">
<li><span class="math inline">B_0 = 0</span>,</li>
<li><span class="math inline">\{ B_t \}_{t \geq 0}</span> is adapted to
<span class="math inline">\{ \mathcal F_t \}_{t \geq 0}</span>,</li>
<li>if <span class="math inline">s &lt; t</span> then <span
class="math inline">B_t - B_s</span> is independent of <span
class="math inline">\mathcal F_s</span>,</li>
<li><span class="math inline">B_t - B_s \sim N(0, t-s)</span>,</li>
<li>and with probability 1 <span class="math inline">t \mapsto
B_t</span> is continuous.</li>
</ul>
<p><em>Def</em>: A random variable <span class="math inline">\tau</span>
taking values in <span class="math inline">[0, \infty]</span> is called
a stopping time with respect to <span class="math inline">\{ \mathcal
F_t \}_{t \geq 0}</span> if for every <span
class="math inline">t</span>, the event <span class="math inline">\{\tau
\leq t\} \in \mathcal F_t</span>.</p>
<p><em>Examples</em>: The following are all stopping times:</p>
<ul class="incremental">
<li>constants;</li>
<li><span class="math inline">\tau = \inf\{ t \mid B_t \in V \}</span>
where <span class="math inline">V</span> is Borel;</li>
<li><span class="math inline">\tau_1 \land \tau_2, \tau_1 \lor
\tau_2</span>, where <span class="math inline">\tau_1, \tau_2</span> are
both stopping times.</li>
</ul>
<p><em>Def</em>: If <span class="math inline">\tau</span> is a stopping
time, then <span class="math inline">\mathcal F_\tau</span> is the <span
class="math inline">\sigma</span>-algebra corresponding to the
collection of events <span class="math inline">A</span> such that for
each <span class="math inline">t</span>, <span class="math inline">A
\cap \{ \tau \leq t \} \in \mathcal F_t</span>.</p>
<h4 id="the-markov-property-of-brownian-motion">The Markov Property of
Brownian Motion</h4>
<p><em>Def</em>: For some stochastic process <span
class="math inline">\{X_t\}_{t \geq 0}</span> (or any other indexed set)
with filtration <span class="math inline">\{F_t\}_{t \geq 0}</span>,
<span class="math inline">X_t</span> has the <strong>Markov
property</strong> if it saitisfies that <span class="math display">
  E[f(X_t) \mid \mathcal F_s] = E[f(X_t) \mid \sigma(X_s)].
</span></p>
<p><em>Def</em>: In general, if <span class="math inline">X_t</span> is
a stochastic process and <span class="math inline">\tau</span> is a
stopping time, both adapted to <span class="math inline">\{ \mathcal F_t
\}_{t \geq 0}</span> with <span class="math inline">P(\tau &lt; \infty)
= 1</span>, then <span class="math inline">X_t</span> has the
<strong>strong Markov property</strong> if <span
class="math inline">X_{\tau + t}</span> is independent of <span
class="math inline">\mathcal F_{\tau}</span>.</p>
<p><strong>Prop</strong>: Suppose <span class="math inline">B_t</span>
is a Brownian motion and <span class="math inline">\tau</span> is a
stopping time, both with respect to <span class="math inline">\{
\mathcal F_t \}</span>, and assume <span class="math inline">P(\tau &lt;
\infty) = 1</span>. Set <span class="math display">
  Y_t = B_{\tau + t} - B_{\tau}.
</span> Then <span class="math inline">Y_t</span> is a Brownian motion
independent of <span class="math inline">\mathcal F_t</span>,
e.g. Brownian motion has the strong Markov property and the new process
is also a Brownian motion.</p>
<p><em>Proof</em>: You proceed by doing successive approximations.</p>
<ul class="incremental">
<li>First, let <span class="math inline">\tau</span> take a finite
amount of values, and use the normal Markov property.</li>
<li>Then, approximate any <span class="math inline">\tau</span> by
stopping times taking a finite amount of values, such as by <span
class="math display">
\tau_n = \begin{cases}
  \frac{k}{2^n} &amp; \frac{k - 1}{2^n} \leq \tau \leq \frac{k}{2^n}
\leq n \\
  n &amp; \tau &gt; n
\end{cases}
</span> for example.</li>
<li>Take a limit by continuity.</li>
</ul>
<p>In particular, the following is clear for Brownian motion:</p>
<p><strong>Prop</strong>: If <span class="math inline">\{B_t\}_{t \geq
0}</span> is a Brownian motion, <span class="math inline">t</span> a
fixed time, and <span class="math inline">Y_s = B_{t+s} - B_t</span>,
then <span class="math inline">\{Y_s\}_{s \geq 0}</span>, is a BM and
independent of <span class="math inline">\mathcal F_t = \sigma\{B_s \mid
s \leq t\}</span>.</p>
<p><strong>Theorem</strong>: Set <span class="math inline">B_t</span> to
a Brownian motion with drift zero; <strong>the reflection
principle</strong> is that <span class="math display">
  P\left(\max_{0 \leq s \leq t} B_s \geq a\right) = 2P(B_t \geq a).
</span> Set <span class="math inline">\tau_a = \min \{ s \mid B_s = a
\}</span>; we also have <span class="math display">
  P(\tau_a \leq t) = 2P(B_t \geq a)
</span> or equivalently <span class="math display">
  P(B_t \geq a \mid \tau_a \leq t) = \frac{1}{2}.
</span></p>
<p><strong>Prop</strong>: If <span class="math inline">0 &lt; r &lt; s
&lt; \infty</span>, <span class="math display">
  q(r, s) = P(B_t = 0 \text{ for some } r \leq t \leq s) = 1 -
\frac{2}{\pi} \arctan\left(\sqrt{\frac{r}{s - s}}\right).
</span></p>
<p><em>Proof</em>: First, I claim that <span class="math inline">q(r, s)
= q(1, s/r)</span> just by a change of variables, so we only need to
compute <span class="math inline">q(t) = q(1, 1 + t)</span>. Set <span
class="math inline">A = \{ B_s = 0 \text{ for some } 1 \leq s \leq 1 + t
\}</span>, so that <span class="math display">
\begin{align*}
  q(t) &amp;= \frac{2}{\sqrt{2 \pi}}\int_0^\infty P(A \mid B_1 = x)
e^{-x^2 / 2}dx.
\end{align*}
</span> However, by the reflection principle, we have that <span
class="math display">
\begin{align*}
  P(A \mid B_1 = x) &amp;= P\left(\max_{0 \leq s \leq t} B_s \geq x
\right) \\
  &amp;= P \left( \min_{0 \leq s \leq t} B_s \leq -x \right) \\
  &amp;= 2P(B_t \geq x) = 2P\left(B_1 \geq \frac{x}{\sqrt{t}}\right)
\end{align*}
</span> upon which we can just compute the integral.</p>
<p><strong>Corollary</strong>: One dimensional standard Brownian motion
is (pointwise) recurrent (that is, the zero set of Brownian motion is
unbounded).</p>
<p><strong>Corollary</strong>: Since <span class="math inline">Y_t =
t^{-1}B_{1/t}</span> is a standard Brownian motion, this shows that for
any <span class="math inline">\epsilon &gt; 0</span>, <span
class="math inline">Z_\epsilon = \{ t \mid B_t = 0, 0 \leq t \leq
\epsilon \}</span> has more elements that just <span
class="math inline">0</span>.</p>
<h4 id="martingales">Martingales</h4>
<p><em>Def</em>: A process <span class="math inline">\{ M_t \}_{t \geq
0}</span> is a <strong>supermartingale</strong> (resp.
<strong>submartingale</strong>) w.r.t. <span class="math inline">\{
\mathcal F_t \}</span> if - <span class="math inline">E[|M_t|] &lt;
\infty</span>, - <span class="math inline">M_t</span> is <span
class="math inline">\{ \mathcal F_t \}</span> adapted, - and if <span
class="math inline">E[M_t \mid \mathcal F_s] \leq M_s</span> (resp.
<span class="math inline">\geq M_s</span>) almost surely for <span
class="math inline">s \leq t</span>. A process which is both a
submartingale and supermartingale is just a martingale, it is continuous
if <span class="math inline">M_t</span> is a continuous function of
<span class="math inline">t</span> almost surely, and is square
integrable (or simply <span class="math inline">L^2</span>) if it has
finite second moment for all <span class="math inline">t</span>.</p>
<p>As an aside, I’m going to stop saying with probability 1 or almost
surely because it’s annoying!</p>
<p><strong>Prop</strong>: Brownian motion (without drift) is an <span
class="math inline">L^2</span> continuous martingale.</p>
<p><strong>Theorem (Kolmogorov Zero-One)</strong>: Tail events of
independent <span class="math inline">\sigma</span>-algebras happen with
probability 0 or 1.</p>
<p><em>Proof</em>: Set <span class="math inline">\mathcal F_n =
\sigma(X_1, \dots, X_n)</span>, and <span class="math inline">\mathcal
T_n = \sigma(X_{n+1}, \dots)</span>, <span class="math inline">\mathcal
F_\infty = \bigcup_{n} \mathcal F_n</span>, and <span
class="math inline">\mathcal T_\infty = \bigcap_n \mathcal
T_n</span>.</p>
<p>Then, one can see that if <span class="math inline">A \in \mathcal
F_\infty</span> and <span class="math inline">\epsilon &gt; 0</span>,
there is <span class="math inline">n</span> and <span
class="math inline">A_n \in \mathcal F_n</span> such that <span
class="math inline">P(A_n \Delta A) &lt; \epsilon</span>; even better,
there exists <span class="math inline">A_n</span> independent of <span
class="math inline">A \in \mathcal T_\infty</span> such that <span
class="math inline">P(A \Delta A_n) &lt; \epsilon</span>, and so we
conclude that <span class="math inline">P(A) = P(A)P(A)</span>.</p>
<p><strong>Theorem (Blumenthal Zero-One)</strong>: Let <span
class="math inline">B_t</span> be a Brownian motion with the standard
filtration, and set <span class="math inline">\mathcal F_{0+} =
\bigcap_{\epsilon &gt; 0} \mathcal F_{\epsilon}</span>; then, if <span
class="math inline">A \in \mathcal F_{0+}</span>, either <span
class="math inline">P(A) = 0</span> or <span
class="math inline">1</span>.</p>
<h4 id="quadratic-variation">Quadratic Variation</h4>
<p>Let <span class="math inline">B_t</span> be a standard Brownian
motion; a partition <span class="math inline">\Pi</span> of <span
class="math inline">[0, 1]</span> is a sequence <span
class="math inline">0 = t_0 &lt; t_1 &lt; \dots &lt; t_k = 1</span>, and
the mesh of the partition is just <span class="math display">
  \| \Pi \| = \max_{i = 1, \dots, k} t_i - t_{i - 1}.
</span> Now take a sequence of partitions <span
class="math inline">\Pi_n</span>, e.g. <span class="math inline">0 =
t_{0, n} &lt; \dots &lt; t_{k_n, n}</span>, and define the quantity
<span class="math display">
  Q(t, \Pi) = \sum_{t_{j} &lt;= t} (B_{t_j} - B_{t_{j-1}})^2
</span> and <span class="math inline">Q_n(t) = Q(t, \Pi_n)</span> and
<span class="math inline">Q_n = Q_n(1)</span>.</p>
<p><strong>Theorem</strong>: If <span class="math inline">\| \Pi_n \|
\to 0</span>, then <span class="math inline">Q_n \to 1</span> in
probability. Furthermore, if <span class="math inline">\sum_{n=1}^\infty
\| \Pi_n \| &lt; \infty</span>, then almost surely <span
class="math inline">\lim_{n \to \infty} Q_n = 1</span>.</p>
<p><em>Proof</em>: A simple computation gives us that <span
class="math display">
  E(Q_n) = \sum_{j=1}^{k_n}E[(B_{t_j} - B_{t_{j-1}})^2] =
\sum_{j=1}^{k_n}(t_j - t_{j-1}) = 1
</span> and <span class="math display">
  \mathop{\mathrm{Var}}(Q_n) = \sum_{i=1}^{k_n}
\mathop{\mathrm{Var}}((B_{t_j}  - B_{t_{j-1}})^2) = \sum_{j=1}^{k_n}
(t_j - t_{j-1})^2 \mathop{\mathrm{Var}}(B_1^2) = 2\sum_{j=1}^{k_n}(t_j -
t_{j-1})^2.
</span> Then, <span class="math display">
  \mathop{\mathrm{Var}}(Q_n) \leq \| \Pi_n \| 2 \sum_{j=1}^{k_n}(t_j -
t_{j-1}) = 2 \| \Pi_n \|
</span> and <span class="math display">
  P(|Q_n - 1| \geq \epsilon) \leq
\frac{\mathop{\mathrm{Var}}(Q_n)}{\epsilon^2} \leq \frac{2 \| \Pi_n
\|}{\epsilon^2}.
</span></p>
<p>The latter half of the theorem follows from Borel-Cantelli.</p>
<p><strong>Theorem</strong>: In general, if <span
class="math inline">\sum_{n=1}^\infty \| \Pi_n \| &lt; \infty</span>,
then almost surely we have <span class="math inline">Q_n(t) \to
t</span>.</p>
<p><em>Proof</em>: With probability 1 this holds for rational <span
class="math inline">t</span>, but by construction <span
class="math inline">Q_n(t)</span> is monotone, so it holds
everywhere.</p>
<p><em>Def</em>: In general, if <span class="math inline">X_t</span> is
a process, then its quadratic variation is <span class="math display">
  \left\langle X\right\rangle_t = \lim_{n \to \infty} \sum_{t_{j,n} \leq
t} (X_{t_{j, n}} - X_{t_{j-1, n}})^2
</span> (sort of, since sometimes this depends on the partition).</p>
<p><em>Def</em>: Alternatively, if <span class="math inline">M_t</span>
is a continuous <span class="math inline">L^2</span>-martingale, its
quadratic variation is the unique predictable process <span
class="math inline">\left\langle M \right\rangle_t</span> that makes
<span class="math inline">M_t^2 - \left\langle M \right\rangle_t</span>
a martingale (in particular, this exists by Doob decomposition).</p>
<p>We showed above that <span class="math inline">\left\langle B_t
\right\rangle_t = t</span>.</p>
<p><strong>Prop</strong>: If <span class="math inline">B_t</span> is a
standard Brownian motion and <span class="math inline">Y_t = \mu t +
\sigma B_t</span>, then <span class="math display">
  \left\langle Y \right\rangle_t = \sigma^2 t.
</span></p>
<p><em>Proof</em>: Just check directly.</p>
<h4 id="law-of-the-iterated-logarithm">Law of the Iterated
Logarithm</h4>
<p><strong>Lemma (Relaxed Borel-Cantelli)</strong>: Let <span
class="math inline">A_1, A_2, \dots</span> be a sequence of events, and
set <span class="math inline">\mathcal F_n = \sigma(A_1, \dots,
A_n)</span>; if there is <span class="math inline">q_n</span> with <span
class="math display">
  \sum_{n=1}^\infty q_n = \infty
</span> such that <span class="math inline">P(A_n \mid \mathcal F_{n-1})
\geq q_n</span>, then <span class="math inline">A_n</span> happens
infinitely often almost surely.</p>
<p><em>Proof</em>: Same as usual, but just with a little more
caution.</p>
<p>Recall that the second Borel-Cantelli lemma tells us that if they are
independent and <span class="math inline">\sum_{n=1}^\infty P(A_n) =
\infty</span>, then <span class="math inline">A_n</span> occurs
infinitely often with probability 1; this lemma is stronger than
that.</p>
<p><strong>Theorem</strong>: If <span class="math inline">B_t</span> is
a standard Brownian motion, then <span class="math display">
  \limsup_{t \to \infty} \frac{B_t}{\sqrt{2 t \log \log t}} = 1.
</span></p>
<p><strong>Corollary</strong>: By symmetry, <span class="math display">
  \liminf_{t \to \infty} \frac{B_t}{\sqrt{2 t \log \log t}} = -1.
</span></p>
<p><em>Proof</em>: Let <span class="math inline">\mathcal T_t = \sigma\{
B_{s + t} - B_t \mid s \geq 0\}</span>, and <span
class="math inline">\mathcal T_\infty = \bigcap_t \mathcal T_t</span>;
one can adapt the arguments from the Kolmogorov 0-1 law to show that
everything in <span class="math inline">\mathcal T_\infty</span> happens
with probability 0 or 1. Then, <span class="math display">
  A_\epsilon = \left\{ \omega \mid \limsup_{t \to \infty}
\frac{B_t}{\sqrt{2 t (1 + \epsilon) \log \log t}} \leq 1\right\}
</span> is a tail event (e.g. in <span class="math inline">\mathcal
T_\infty</span>) and thus <span class="math inline">P(A_\epsilon) =
0</span> or <span class="math inline">1</span>. In fact, if <span
class="math inline">\epsilon &lt; 0</span> then it’s 0, and if <span
class="math inline">\epsilon &gt; 0</span> then it’s 1. In fact, by this
0-1 law and symmetry one may see <span class="math display">
  P \left( \limsup_{t \to \infty} \frac{|B_t|}{\sqrt{2 t (1 + \epsilon)
\log \log t}} \leq 1 \right) = P \left(\limsup_{t \to \infty}
\frac{B_t}{\sqrt{2 t (1 + \epsilon) \log \log t}} \leq 1\right)
</span> as well.</p>
<p>First take <span class="math inline">\epsilon &gt; 0</span> and take
some <span class="math inline">\rho &gt; 1</span> to be specified later.
Then, let <span class="math display">
  V_n = \left\{ |B_{\rho^n}| \geq \sqrt{2 \rho^n(1-\epsilon) \log \log
\rho^n} \right\};
</span> we want to to show that <span class="math inline">V_n</span>
occurs infinitely often. I claim that <span class="math display">
  P(V_{n+1} \mid V_1, \dots, V_n) \geq P \left( B_{\rho^{n+1}} -
B_{\rho^n} \geq \sqrt{2 \rho^{n+1}(1-\epsilon) \log \log \rho^n}
\right).
</span> To see this, compute <span class="math display">
\begin{align*}
  &amp;P \left( \frac{B_{\rho^{n+1}} - B_{\rho^n}}{\sqrt{\rho^{n+1} -
\rho^n}} \geq \frac{\sqrt{2\rho^{n+1}(1-\epsilon)\log \log
\rho^n}}{\sqrt{\rho^n (\rho - 1)}} \right) \\
  &amp;= P \left( \frac{B_{\rho^{n+1}} - B_{\rho^n}}{\sqrt{\rho^{n+1} -
\rho^n}} \geq \sqrt{\frac{2 \rho}{\rho - 1}(1-\epsilon)(\log n + \log
\log \rho)} \right) \\
\end{align*}
</span> and choose <span class="math inline">\rho</span> large enough so
that <span class="math inline">\frac{2 \rho}{\rho - 1}(1-\epsilon) &lt;
1</span>, and use the estimate <span class="math inline">P(B_1 &gt; x)
\sim \exp(-x^2 / 2)</span> and conclude by the earlier lemma. The other
direction for <span class="math inline">\epsilon &lt; 0</span> is
similar (in fact, easier since we may conclude from the first
Borel-Cantelli lemma).</p>
<h4 id="zero-sets-of-brownian-motion">Zero Sets of Brownian Motion</h4>
<p><em>Def</em>: Set <span class="math inline">B_t</span> a standard
Brownian motion, <span class="math inline">Z = \{ t \mid B_t = 0
\}</span>, and <span class="math inline">Z_t = Z \cap [0, t]</span>.
Then, <span class="math inline">t \in Z</span> is right-isolated if
<span class="math inline">t \in Z</span>, and <span
class="math inline">\exists \epsilon &gt; 0</span> such that <span
class="math inline">(t, t + \epsilon) \cap Z = \emptyset</span>; similar
for left-isolated. A point which is both left and right isolated is just
isolated.</p>
<p>With probability <span class="math inline">1</span>, <span
class="math inline">0</span> is not right-isolated; further, <span
class="math inline">Z_1</span> is homeomorphic to the Cantor set.</p>
<p>With probability 1, the sets of left and right isolated points are
countable, and there are no isolated points.</p>
<p>We can show that if <span class="math inline">q \in \mathbb{Q}_{\geq
0}</span>, <span class="math inline">P(B_q  = 0) = 0</span>, and <span
class="math inline">\tau_q = \min \{ t \geq q \mid B_t = 0\}</span> is a
stopping time; further, the left-isolated points are just <span
class="math inline">\{ \tau_q \mid q \in \mathbb{Q}_{\geq 0}\}</span>.
And by strong Markov property, no <span
class="math inline">\tau_q</span> is a right-isolated point, so there
are no isolated points.</p>
<p>Set <span class="math inline">\sigma_q = \max_t \{ t &lt; q \mid B_t
= 0 \}</span> (this is well-defined, but not a stopping time). Then
associate every <span class="math inline">q</span> to an interval <span
class="math inline">(\sigma_q, \tau_q)</span>; then <span
class="math inline">Z = [0, \infty) \setminus \bigcup_q (\sigma_q,
\tau_q)</span> and has Lebesgue measure 0. To see this, we just
interchange integrals: <span class="math display">
  E[ \lambda(Z_1) ] = E\left[ \int_0^1 1_{\{B_s = 0\}} ds \right] =
\int_0^1 P\{ B_s = 0 \}ds = 0.
</span></p>
<p>Look at <span class="math inline">Z \cap [1, 2]</span>; now cover
<span class="math inline">[1, 2]</span> by intervals of length <span
class="math inline">n^{-1}</span>; put the number of these intervals
that intersect <span class="math inline">Z</span> as <span
class="math inline">X_n</span>. Then, <span class="math display">
  E[X_n] = \sum_{j=1}^n P\left(Z \cap \left[1 + \frac{j-1}{n}, 1 +
\frac{j}{n}  \right]\neq \emptyset \right) \sim Cn^{1/2}.
</span></p>
<p>The box or Minkowski dimension of a set is given by the exponent
above (sort of).</p>
<p><em>Def</em>: The <strong>Hausdorff dimension</strong> comes from the
<strong>Hausdorff measure</strong>: for <span
class="math inline">\epsilon &gt; 0</span> and <span
class="math inline">\alpha</span>, set <span class="math display">
  \mathcal H^\alpha_\epsilon = \left\{ \inf \sum_{j=1}^\infty
(\operatorname{diam} U_j)^\alpha \right\}
</span> where the infimum is over all coverings <span
class="math inline">\bigcup_j=1^\infty U_j</span> of <span
class="math inline">V</span> with each <span
class="math inline">\operatorname{diam} (U_j) &lt; \epsilon</span>. Then
the Hausdorff measure is given by <span class="math display">
  \mathcal H^\alpha(V) = \lim_{\epsilon \to 0} \mathcal
H_{\epsilon}^\alpha (V).
</span> Then, there is some number <span class="math inline">D</span>
such that <span class="math display">
  \mathcal H^{\alpha}(V) = \begin{cases}
    \infty &amp; \alpha &lt; D \\
    0 &amp; \alpha &gt; D
  \end{cases}
</span> and we call this <span class="math inline">D</span> the
Hausdorff dimension. In general, the Hausdorff dimension is at most the
Minkowski dimension, but in this case we actually do have equality.</p>
<h4 id="local-time">Local Time</h4>
<p>The local time is the amount of time the Brownian motion spends at
<span class="math inline">0</span> by a certain time. It is sort of like
the Cantor measure, insofar as if <span class="math inline">s &lt;
t</span>, then <span class="math inline">L_t - L_s &gt; 0 \iff (s, t)
\cap Z \neq \emptyset</span>.</p>
<p><em>Def</em>: We define the <strong>local time</strong> of Brownian
motion at <span class="math inline">x \in \mathbb{R}</span>, <span
class="math inline">L_t</span>, by first setting <span
class="math display">
  L_{t, \epsilon}(x) = \frac{1}{2\epsilon} \int_0^t 1_{\{|B_s - x| \leq
\epsilon\}} ds
</span> and then letting <span class="math inline">L_t(x) =
\lim_{\epsilon \to 0} L_{t, \epsilon}</span>. We abbreviate <span
class="math inline">L_t = L_t(0)</span>.</p>
<p>We can compute the expectation <span class="math display">
  \begin{align*}
    E[L_{t, \epsilon}] &amp;= \frac{1}{2\epsilon} \int_0^t P(|B_s| \leq
\epsilon)ds \\
    &amp;\sim \frac{1}{2\epsilon} \int_0^t 2e \frac{1}{2 \pi s}ds \\
    &amp;= \sqrt{\frac{2}{\pi}}t^{1/2}
  \end{align*}.
</span></p>
<p><strong>Theorem</strong>: With probability 1, <span
class="math inline">L_t</span> exists for all <span
class="math inline">t</span>, and this holds in <span
class="math inline">L^2</span> as well.</p>
<p>There are more facts: <span class="math inline">L_t</span> is
continuous in <span class="math inline">t</span> and non-decreasing,
<span class="math inline">L_t - L_s &gt; 0 \iff (s, t) \cap Z \neq
\emptyset</span>, and <span class="math inline">L_t</span> is weakly
<span class="math inline">\frac{1}{2}</span>-Hölder continuous.</p>
<p><strong>Theorem (Scaling Rule)</strong>: <span
class="math inline">L_t</span> has the same distribution as <span
class="math inline">t^{1/2} L_1</span>. Further, <span
class="math inline">M_t = \max_{ 0 \leq s \leq t}B_s</span> has the same
distribution as <span class="math inline">L_t</span>.</p>
<h3 id="brownian-motion-in-several-dimensions">Brownian Motion in
Several Dimensions</h3>
<hr />
<p><em>Def</em>: If <span class="math inline">B_t^1, \dots, B_t^d</span>
are independent standard Brownian motions, then <span
class="math display">
  B_t = (B_t^1, \dots, B_t^d)
</span> is a standard Brownian motion in <span
class="math inline">\mathbb{R}^d</span>.</p>
<p><strong>Prop</strong>: <span class="math inline">B_0 = \mathbf
0</span>, has independent increments, if <span class="math inline">s
&lt; t, B_t - B_s \sim N(0, (t-s)I)</span>, and <span
class="math inline">B_t</span> has continuous paths.</p>
<p>In particular, if we have the above with <span
class="math inline">B_t - B_s \sim N(\mu, \Gamma)</span>, then <span
class="math inline">B_t</span> is a Brownian motion with dift <span
class="math inline">\mu \in \mathbb{R}^d</span> and covariance matrix
<span class="math inline">\Gamma</span>. Further, if <span
class="math inline">AA^T = \Gamma</span>, then we may write <span
class="math inline">B_t = AY_t + t \mu</span> for a standard Brownian
motion <span class="math inline">Y_t</span>.</p>
<p>Consider the open annulus with inner radius <span
class="math inline">r</span> and outer radius <span
class="math inline">R</span>, e.g. <span class="math inline">D(r, R) =
\{ y \in \mathbb{R}^d \mid r &lt; |y| &lt; R \}</span>. Start a Brownian
motion at <span class="math inline">x</span>, and let <span
class="math inline">\tau = \tau(r, R)</span> be the first time with
<span class="math inline">|B_t| = r</span> or <span
class="math inline">|B_t| = R</span>. What is the probability that <span
class="math inline">|B_\tau| = R</span>?</p>
<p>In one dimension, this is easy: stop the Brownian motion at <span
class="math inline">\tau</span> and look at what happens at
infinity.</p>
<p>In higher dimensions, we need a quick detour.</p>
<h4 id="harmonic-functions-in-mathbbrd">Harmonic Functions in <span
class="math inline">\mathbb{R}^d</span></h4>
<p>For this section, a domain is an open connected subset of <span
class="math inline">\mathbb{R}^d</span>.</p>
<p><em>Def</em>: For a domain <span class="math inline">D</span>, we say
<span class="math inline">f: D \to \mathbb{R}</span> is harmonic if it
is continuous (or merely locally integrable) and satisfies the mean
value property <span class="math display">
  f(x) = MV(f, x, \epsilon) = \int_{|x - y| = \epsilon} f(y) ds
</span> where <span class="math inline">s</span> is the surface measure,
normalized so that <span class="math inline">\int_{|x - y| = \epsilon}ds
= 1</span>.</p>
<p>In a probabilistic vein, if <span class="math inline">B_t</span> is a
standard <span class="math inline">d</span>-dimensional Brownian motion
starting at <span class="math inline">x</span>, and <span
class="math inline">\tau = \min \{ t \mid |B_t - x| = \epsilon
\}</span>, since <span class="math inline">B_t</span> is rotation
invariant, the above is just <span class="math display">
  MV(f, x, \epsilon) = E[f(B_\tau)].
</span></p>
<p><em>Def</em>: The Laplacian of <span class="math inline">f</span> is
<span class="math display">
  \nabla f = \sum_{j=1}^d \frac{\partial ^2 f }{\partial x_j^2}.
</span></p>
<p><strong>Prop</strong>: If <span class="math inline">f</span> is <span
class="math inline">C^2</span> in <span class="math inline">D</span>,
then <span class="math display">
  \frac{1}{2d}\nabla f = \lim_{\epsilon \to 0} \frac{MV(f, x, \epsilon)
- f(x)}{\epsilon^2}.
</span></p>
<p><em>Proof</em>: Look at the Taylor expansion.</p>
<p><strong>Theorem</strong>: <span class="math inline">f</span> is
harmonic in <span class="math inline">D</span> if and only if it is in
<span class="math inline">C^2</span> and <span
class="math inline">\nabla f = 0</span> everywhere.</p>
<h4 id="hitting-probabilities-for-brownian-motion">Hitting Probabilities
for Brownian Motion</h4>
<p>Let <span class="math inline">\tau = \tau_{r, R} = \min \{ |B_t| = r
\text{ or } R \} = \min \{ t \mid B_t \in \partial D \}</span>. We will
let a superscript <span class="math inline">x</span> denote that <span
class="math inline">B_0 = x</span>, and let <span class="math display">
  \varphi(x) = P^x(|B_\tau| = R).
</span> By rotational invariance, <span class="math inline">\varphi(x) =
\varphi(|x|)</span>, and it is continuous at the boundary; we also
clearly have <span class="math inline">\varphi(x) = 0</span> if <span
class="math inline">|x| = r</span> and <span
class="math inline">\varphi(x) = 1</span> if <span
class="math inline">|x| = R</span>. The strong Markov property furnishes
that <span class="math inline">\varphi</span> is harmonic. Set <span
class="math inline">\tau_\epsilon = \min \{ t \mid |B_t - x| =
\epsilon\}</span>; then <span class="math display">
  P^X(|B_\tau| = R \mid \mathcal F_{\tau_\epsilon}) =
\varphi(B_{\tau_\epsilon})
</span> and <span class="math display">
  P^X(|B_\tau| = R) = E^x[P(|B_\tau| = R \mid \mathcal
F_{\tau_\epsilon})] = E^x[\varphi(B_{\tau_\epsilon})] = MV(\varphi, x,
\epsilon).
</span> This list of properties gives a unique function (solve an ODE in
polar coordinates), and is given by <span class="math display">
  \varphi(x) = \frac{|x|^{2-d} - r^{2-d}}{R^{2-d} - |r|^{2-d}}
</span> for <span class="math inline">d \neq 2</span> and <span
class="math display">
  \varphi(x) = \frac{\log|x| - \log r}{\log R - \log r}
</span> for <span class="math inline">d = 2</span>.</p>
<h4 id="recurrence-and-transience-of-brownian-motion">Recurrence and
Transience of Brownian Motion</h4>
<p>Let <span class="math inline">B_t</span> be a standard Brownian
motion in <span class="math inline">\mathbb{R}^d</span>.</p>
<p><strong>Theorem</strong>: With probability 1, Brownian motion is
transient in dimensions at least 3; that is, <span class="math display">
  \lim_{t \to \infty} |B_t| = \infty.
</span></p>
<p><em>Proof</em>: If <span class="math inline">d \geq 3</span>, and we
start at <span class="math inline">x</span> with <span
class="math inline">|x| &gt; r</span>, if we set <span
class="math inline">T_r = \min \{ t \mid |B_t| = r \}</span>, <span
class="math display">
  P^x(T_r &lt; \infty)  = \lim_{R \to \infty} P^X(|B_{\tau_{r, R}}| = r)
= \left( \frac{r}{|x|} \right)^{d-2} &lt; 1
</span></p>
<p><strong>Theorem</strong>: With probability 1, Brownian motion is
neighborhood recurrent; that is, <span class="math inline">\forall z \in
\mathbb{R}^2, \epsilon &gt; 0</span>, Brownian motion visits the disk of
radius <span class="math inline">\epsilon</span> about <span
class="math inline">z</span> infinitely often. However, it is not point
recurrent, e.g. it hits <span class="math inline">z \neq 0</span> with
probability 0.</p>
<p><em>Proof</em>: If <span class="math inline">d = 2</span>, then <span
class="math display">
  P^x(T_r &lt; \infty) = \lim_{R \to \infty} P(|B_{\tau_{r, R}}| = r) =
1
</span> But if <span class="math inline">T = \min\{ t \mid B_t = 0
\}</span>, <span class="math display">
  P^x(T &lt; \infty) \leq \lim_{R \to \infty} \lim_{r \to 0}
P^x(|B_{\tau_{r, R}}| = r) = 0
</span></p>
<p>A fun fact is that if <span class="math inline">d \geq 2</span>, then
<span class="math inline">\{ B_t, t \geq 0 \}</span> has Hausdorff
dimension <span class="math inline">2</span> but also zero Hausdorff-2
measure.</p>
<h4 id="the-dirchlet-problem">The Dirchlet Problem</h4>
<p>Take a bounded domain <span class="math inline">D \subset
\mathbb{R}^d</span>, and a continuous function <span
class="math inline">F: \partial D \to \mathbb{R}</span>; the Dirichlet
problem is to find the unique continuous <span class="math inline">f:
\overline D \to \mathbb{R}</span> that agrees with <span
class="math inline">F</span> on <span class="math inline">\partial
D</span> and is harmonic on <span class="math inline">D</span>.</p>
<p>In fact, uniqueness follows from the maximum principle, which says
that the maximum of <span class="math inline">f</span> is attained on
the boundary (think about the mean value principle). Then subtract two
solutions and see that it is 0.</p>
<p>Let <span class="math inline">T = \min\{t \mid B_t \in \partial D
\}</span>, and <span class="math inline">f(x) = E^x[F(B_T)]</span>. This
satisfies the mean value principle, and in fact continuous (which is
kinda hard) but it is obviously locally integrable, so <span
class="math inline">f</span> here is harmonic.</p>
<p>In general, such a harmonic function is not necessarily continuous on
the boundary: take the example of the punctured unit disk, with <span
class="math inline">F(x) = 1</span> for <span class="math inline">|x| =
1</span> and <span class="math inline">F(0) = 0</span>. And so
everything is fine except at the origin, since <span
class="math inline">f(x) = P^x(|B_T| = 1) = 1</span>.</p>
<p><em>Def</em>: If <span class="math inline">x \in \partial D</span>,
let <span class="math inline">\sigma = \inf \{ t &gt; 0 \mid B_t \in
\partial D\}</span>; <span class="math inline">x</span> is regular if
<span class="math inline">P^x(\sigma = 0) = 1</span>.</p>
<p><strong>Prop</strong>: <span class="math inline">f</span> as defined
above is continuous at every regular boundary point. If we relax the
boundary to be only bounded and measurable, then the above is continuous
at every regular point at which <span class="math inline">F</span> is
continuous.</p>
<p>Therefore the Dirichlet problem has a solution for every continuous
<span class="math inline">F</span> if and only if every point on <span
class="math inline">\partial D</span> is regular.</p>
<p><em>Def</em>: If <span class="math inline">D</span> is a domain, then
<strong>harmonic measure</strong> on <span class="math inline">D</span>
(or on <span class="math inline">\partial D</span>) at <span
class="math inline">z \in D</span>, is the hitting measure of Brownian
motion, starting at <span class="math inline">z</span> and stopped at
<span class="math inline">\partial D</span>. That is, if <span
class="math inline">V \subset \partial D</span>, <span
class="math display">
  \mathop{\mathrm{hm}}_D(V, z)  = P^z(B_T \in V).
</span></p>
<p><strong>Prop</strong>: Note that <span
class="math inline">\mathop{\mathrm{hm}}_D(\partial D, z) = P^z(T &lt;
\infty)</span> so if <span class="math inline">D</span> is bounded, this
is a probability measure. Then, <span class="math display">
  E^z[F(B_T)] = \int_{\partial D} F(w) \mathop{\mathrm{hm}}_D(dw, z)
</span> and if <span class="math inline">\partial D</span> is smooth,
then <span class="math inline">\mathop{\mathrm{hm}}_D(\cdot, z)</span>
is absolutely continuous with respect to surface measure; if <span
class="math inline">H_D(z, w)</span> is the Poisson kernel, <span
class="math display">
  \mathop{\mathrm{hm}}_D(v, z) = \int_V H_D(z, w)S(dw)
</span> where <span class="math inline">S</span> is the surface
measure.</p>
<p>A nice example of all of the above is the unit ball. Then, <span
class="math inline">H_D(z, w) = c_d^{-1}\frac{1 - |z|^2}{|z -
w|^d}</span> where <span class="math inline">c_d</span> is the surface
measure of the unit sphere.</p>
<p><strong>Prop</strong>: The following are true.</p>
<ul class="incremental">
<li>If <span class="math inline">V \subset \partial D</span> and <span
class="math inline">h(z) = \mathop{\mathrm{hm}}_D(V, z) = P^z(B_T \in
V)</span>, then <span class="math inline">h</span> is the unique
harmonic function in <span class="math inline">D</span> with boundary
condition <span class="math display">
F(w) = \begin{cases}
  1 &amp; w \in V \\
  0 &amp; w \notin V
\end{cases}
</span> In particular, one could define the harmonic measure this way
and show existence, but this is unreasonably hard compared to just using
the Brownian motion.</li>
<li>If <span class="math inline">D</span> contains the closed unit ball
<span class="math inline">B</span>, and <span
class="math inline">f</span> is harmonic in <span
class="math inline">D</span>, then for every <span
class="math inline">|z| &lt; 1</span>, <span class="math display">
f(z) = \int_{|w| = 1} f(w) H_B(z, w)S(dw).
</span></li>
</ul>
<p><strong>Corollary (Harnack Inequality)</strong>: For every <span
class="math inline">r \in (0, 1)</span>, there is <span
class="math inline">C = C(r, d)</span> such that if <span
class="math inline">f: B \to (0, \infty)</span> is harmonic, then for
all <span class="math inline">|z|, |z&#39;| \leq r</span>, <span
class="math display">
  f(z) \leq Cf(z&#39;)
</span> and <span class="math display">
  C = \max_{\substack{|z|, |z&#39;| \leq r \\ |w| = 1}} \frac{H_B(z,
w)}{H_B(z&#39;, w)}
</span></p>
<p><strong>Corollary</strong>: For every <span
class="math inline">k</span> there exists <span class="math inline">c =
c(k, d)</span> such that if <span class="math inline">f: B \to
\mathbb{R}</span> is harmonic, then for any <span
class="math inline">k</span>-th order partial derivative, <span
class="math display">
  |\partial^k f(0)| \leq c\| f \|_\infty.
</span> Moreover, there is <span class="math inline">C = C(k, d)</span>
such that if <span class="math inline">f: D \to \mathbb{R}</span> is
harmonic and <span class="math inline">z \in D</span>, then <span
class="math display">
  |\partial^k f(z)| \leq \frac{C_k}{(\operatorname{dist}(z, \partial
D)^k} \left( \max_{|z - w| \leq \operatorname{dist}(z, \partial D) / 2}
|f(w)| \right).
</span></p>
<p><strong>Theorem (Harnack principle)</strong>: if <span
class="math inline">D</span> is a domain and <span class="math inline">K
\subset D</span> is compact, then there exists some <span
class="math inline">C = C(K, D)</span> such that if <span
class="math inline">f: D \to (0, \infty)</span> is harmonic and <span
class="math inline">z, w \in K</span>, then <span class="math display">
  f(z) \leq Cf(w).
</span></p>
<p>Now let <span class="math inline">D</span> and <span
class="math inline">\mathbb{R}^{d} \setminus D = K</span>; if <span
class="math inline">F: K \to \mathbb{R}</span> is continuous, we can ask
about the existence of <span class="math inline">f: \mathbb{R}^d \to
\mathbb{R}</span> which is harmonic on <span
class="math inline">D</span>, coincides with <span
class="math inline">F</span> on <span class="math inline">K</span>, and
is continuous on <span class="math inline">\mathbb{R}^d</span>. Such a
thing is obviously not unique (think!).</p>
<p>Let both <span class="math inline">F</span> and <span
class="math inline">f</span> bounded but otherwise as above. Set <span
class="math inline">T = \min\{ t \geq 0 \mid B_t \in K \}</span>, and
assume for every <span class="math inline">z \in D</span>, <span
class="math inline">P^z(T &lt; \infty) &gt; 0</span>. If <span
class="math inline">d = 1</span> or <span class="math inline">2</span>,
there is a unique <span class="math inline">f</span>, given by <span
class="math inline">f(z) = E^z[f(B_T)]</span>.</p>
<p>Now if <span class="math inline">d \geq 3</span> and <span
class="math inline">K</span> is bounded, then <span
class="math inline">g(z) = P^z(T = \infty) &gt; 0</span>; <span
class="math inline">g</span> is harmonic and bounded, and if <span
class="math inline">g = 0</span> on <span class="math inline">K</span>
then <span class="math inline">g</span> is continuous.</p>
<p><strong>Theorem</strong>: All solutions to the above problem are
given by <span class="math display">
  f(z) = E^z[(F(B_t) 1_{\{T &lt; \infty\}}] + cP^z(T = \infty).
</span></p>
<p>Intuitively, we just add a point at infinity which has value <span
class="math inline">c</span> and get this formula; but this only works
on <span class="math inline">\mathbb{R}^d</span> and <span
class="math inline">\mathbb{Z}^d</span>.</p>
<p>Consider a random walk on an infinite binary tree; such a walk is
clearly recurrent, since it moves away from the starting point with
probability <span class="math inline">2/3</span>. But now there are lots
of infinities.</p>
<h3 id="differential-equations">Differential Equations</h3>
<p>The heat equation is described by some initial function <span
class="math inline">u_0: D \to \mathbb{R}</span>, some boundary
condition <span class="math inline">u(t,x) = F(x)</span> for <span
class="math inline">x \in \partial D</span>, and <span
class="math inline">\dot i(t,x) = \frac{1}{2} \Delta u(t,x)</span>.
Then, <span class="math display">
    u_t(x) = E^x[1_{\{T \leq t\}}F(B_T) + u_0(B_t)1_{\{T &gt; t\}}].
</span></p>
<p>We can also handle Green’s functions. Let <span
class="math inline">B_t</span> be a Brownian motion in <span
class="math inline">\mathbb{R}^d</span>, <span class="math inline">d
\geq 3</span>; <span class="math inline">G(x,y)</span> is the “expected
amount of time spent at <span class="math inline">y</span> starting at
<span class="math inline">x</span>; that is, <span class="math display">
    G(x, y) = \int_0^\infty P_t(x,y)dt = G(y,x) = G(0, y-x).
</span> Further, <span class="math display">
    G(x) = G(0, x) = \int_0^\infty \frac{1}{(2\pi
t)^{d/2}}\exp(-|x|^2/2t)dt;
</span> and when one computes this integral, we get <span
class="math inline">C_d |x|^{2-d}</span> away from the origin. This is
no coincidence: in <span class="math inline">d=2</span>, you get <span
class="math inline">a(x) = C_2\log(|x|)</span>, which is also the unique
radially symmetric harmonic function.</p>
<p>In a domain <span class="math inline">D</span> (in any dimension),
let <span class="math display">
    G_D(x,y) = \int_0^\infty P_t^D(x,y)dt
</span> so in <span class="math inline">d \geq 3</span>, <span
class="math display">
    G(x,y) - E^x[G(B_T, y)]
</span> and in <span class="math inline">d = 2</span>, <span
class="math display">
    E^x[a(B_T, y)] - a(x,y)
</span> where <span class="math inline">a(x,y) = a(y - x)</span>.</p>
<p>For fixed <span class="math inline">y</span>, the function <span
class="math inline">h(x) = G_D(x,y)</span> is harmonic in <span
class="math inline">D \setminus \{ y \}</span> as <span
class="math inline">x \to y</span>.</p>
<h3 id="stochastic-integration">Stochastic Integration</h3>
<p>Let <span class="math inline">\frac{d}{dt} F(t) = C(t, F(t))</span>;
we then write <span class="math inline">dF(t) = C(t, F(t))dt</span>, so
<span class="math inline">F(t) = F(0) + \int_0^t C(s, F(s))ds</span>.
Here we are interested in the case where we allow things to be random,
e.g. <span class="math display">
    dX_t = R_tdt + A_tdB_t.
</span> Intuitively, we require that <span
class="math inline">X_t</span> looks like a BM with some drift <span
class="math inline">R_t</span> and variance <span
class="math inline">A_t^2</span>. Then, <span class="math display">
    X_t = X_0 + \int_0^tR_sds + \int_0^tA_sdB_s.
</span> Of course, we still need to define the (Itô) stochastic
integral.</p>
<p>Let <span class="math inline">B_t</span> be a Brownian motion with a
filtration <span class="math inline">\mathcal F_t</span>.</p>
<p><em>Def</em>: A process <span class="math inline">A_t</span> is a
<strong>simple process</strong> (with respect to <span
class="math inline">\mathcal F_t</span>) if there exists a finite number
of times <span class="math inline">0 = t_0 &lt; t_1 &lt; \dots &lt; t_n
&lt; t_{n+1} = \infty</span> and random variables <span
class="math inline">Y_0, Y_1, \dots, Y_n</span> such that <span
class="math inline">Y_j</span> is <span class="math inline">\mathcal
F_{t_j}</span>-measurable, and <span class="math inline">A_t =
Y_j</span> on <span class="math inline">t_j \leq t &lt; t_{j+1}</span>.
We can have <span class="math inline">L^2</span> or bounded simple
processes, and those are just requirements on the <span
class="math inline">Y_i</span>.</p>
<p><em>Def</em>: If <span class="math inline">A_t</span> is a simple
process, we define the stochastic integral of <span
class="math inline">A_t</span> to be <span class="math display">
    Z_t = \int_0^t A_sdB_s = \sum_{k=0}^{j-1}Y_k[B_{t_{k+1}} - B_{t_k}]
+ Y_j[B_t -  B_{t_j}].
</span></p>
<p>We have some properties immediately:</p>
<ul class="incremental">
<li>If <span class="math inline">A_t</span> and <span
class="math inline">C_t</span> are simple, and <span
class="math inline">a, c \in \mathbb{R}</span>, then <span
class="math inline">aA_t + cC_t</span> is simple and <span
class="math display">
  \int_0^t (aA_t + cC_t)dB_s = a\int_0^tA_tdB_s + c\int_0^tC_tdB_s.
</span></li>
<li>If <span class="math inline">A_t</span> is <span
class="math inline">L^1</span>, then <span
class="math inline">Z_t</span> is a martingale (just compute).</li>
<li>The Itô isometry is, if <span class="math inline">A_t</span> is in
<span class="math inline">L^2</span>, <span class="math display">
  \mathop{\mathrm{Var}}(Z_t) = E[Z_t^2] = \int_0^t E[A_s^2]ds = E \left[
\int_0^t A_s^2ds \right].
</span> Again, just compute and use the orthogonality of martingale
increments.</li>
<li>With probability 1, <span class="math inline">t \mapsto Z_t</span>
is a continuous function.</li>
</ul>
<p>Now let <span class="math inline">A_t</span> be a bounded, continuous
process, adapted to <span class="math inline">\mathcal F_t</span>.
<em>Def</em>: We define <span class="math display">
    \int_0^t A_sdB_s = \lim_{n \to \infty}\int_0^t A_s^{(n)} dB_s
</span> where <span class="math inline">A_s^{(n)}</span> is a sequence
of simple processes approaching <span
class="math inline">A_s</span>.</p>
<p><strong>Lemma</strong>: For every <span class="math inline">t</span>,
we can find a sequence of simple processes <span
class="math inline">A_t^{(n)}</span> with <span
class="math inline">|A_t^{(n)} \leq K</span> and such that <span
class="math display">
    \lim_{n \to \infty} \int_0^t E[(A_s^{(n)} - A_s)^2]ds = 0.
</span></p>
<p><em>Proof</em>: For ease, take <span class="math inline">t =
1</span>. Then, <span class="math display">
    A_t^{(n)} = n \int_{\frac{k-1}{n}}^{\frac{k}{n}}A_sds
</span> does the job. Scale appropriately.</p>
<p>Now if <span class="math inline">Z_t^{(n)} = \int_0^t
A_s^{(n)}dB_s</span>, then <span class="math inline">E[(Z_t^{(n)} -
Z_t^{(m)})^2] = \int_0^tE[(A_s^{(n)} - A_s^{(m)})^2]ds</span>, so <span
class="math inline">Z_t^{(n)}</span> is a Cauchy sequence in <span
class="math inline">L^2(\Omega)</span>, and there is therefore an <span
class="math inline">L^2</span> limit that we call <span
class="math inline">Z_t</span>. Further, there is a continuous
modification of <span class="math inline">Z_t</span>, which we can show
exists by defining it on dyadics and using continuity.</p>
<p><strong>Prop</strong>: Let <span class="math inline">Z_t = \int_0^t
A_sdB_s</span>, with <span class="math inline">A_s</span> continuous in
<span class="math inline">L^2</span>. Then the above properties all
hold.</p>
<p>We do not necessarily need continuity. We do need progressive
measurability, e.g. <span class="math inline">\{A_s(\omega)\}</span> is
measurable on <span class="math inline">\Omega \times [0, t]</span>.</p>
<p>If <span class="math inline">A_s</span> is continuous, however,
without any other boundedness assumptions, let <span
class="math inline">\tau_n = \min \{ |A_s| \geq n \}</span>. Then let
<span class="math inline">A_s^{(n)} = A_{s \land \tau_n}</span> so that
<span class="math inline">Z_t^{(n)} = \int_0^t A_s^{(n)}dB_s</span> is
well-defined, and moreover if <span class="math inline">n &gt; \sup_{0
\leq s \leq t} |A_s|</span>, then <span class="math inline">Z_t^{(n)} =
\int_0^t A_sdB_s</span>. Thus, we define <span class="math display">
    Z_t = \int_0^t A_sdB_s = \lim_{n \to \infty}Z_s^{(n)}
</span> which is a pointwise limit in <span
class="math inline">\Omega</span> (so we have to be careful). With this
definition, linearity, continuity, and the Itô isometry holds (allowing
for <span class="math inline">\infty</span> as a possible value), but
<span class="math inline">Z_t</span> is merely a local martingale.</p>
<p><strong>Prop</strong>: If <span class="math inline">M_t</span> is a
continuous square integrable martingale starting from 0, the for all
<span class="math inline">\epsilon &gt; 0</span>, <span
class="math display">
    P \left(\max_{0 \leq s \leq t}|M_s| \geq \epsilon\right) \leq
\frac{E(M_t^2)}{\epsilon^2}.
</span></p>
<p>Now, set <span class="math inline">M_t = Z_t - Z_t^{(n)}</span> is a
continuous square-integrable martingale; thus by the above proposition,
<span class="math display">
    P(\max_{0 \leq t \leq 1} |M_t| \geq \epsilon ) \leq
\frac{E(M_t^2)}{\epsilon^2}.
</span></p>
<p><strong>Theorem</strong>: Suppose <span
class="math inline">A_s</span> is a continuous process with <span
class="math inline">\int_0^1 E[A_s^2)ds &lt; \infty</span>, and <span
class="math inline">\Pi^{(n)}</span> is a sequence of partitions of
<span class="math inline">[0, 1]</span> such that <span
class="math display">
    \sum_{n=1}^\infty \int_0^1 E[|A_t - A_t^{(n)}|^2]dt &lt; \infty.
</span> If <span class="math inline">Y^{(n)} = \max_{0 \leq t \leq
1}|Z_t - Z_t^{(n)}|</span>, then <span class="math inline">Y^{(n)} \to
0</span> with probability 1.</p>
<p><em>Proof</em>: Apply Borel-Cantelli to the above.</p>
<p>If <span class="math inline">Z_t = \int_0^t A_sdB_s</span>, then the
quadratic variation is <span class="math display">
    \left\langle Z\right\rangle_t = \int_0^t A_s^2ds
</span> and <span class="math inline">Z_t - \left\langle Z
\right\rangle_t</span> is a martingale. In fact, the quadratic variation
is the unique increasing process such that it is a martingale.</p>
<h4 id="itos-formula">Ito’s Formula</h4>
<p><strong>Theorem</strong>: Suppose that <span class="math inline">f:
\mathbb{R}\to \mathbb{R}</span> is <span class="math inline">C^2</span>,
and <span class="math inline">B_t</span> is a standard Brownian motion.
Then <span class="math display">
    f(B_t) - f(B_0) = \int_0^t f&#39;(B_s)dB_s + \frac{1}{2}\int_0^t
f&#39;&#39;(B_s)ds.
</span></p>
<p><em>Proof</em>: Taylor approximate; prove for a countable dense set
of <span class="math inline">t</span>, and conclude for <span
class="math inline">f</span> with compact support. Otherwise, take
something that agrees with <span class="math inline">f</span> on <span
class="math inline">[-n. n]</span> send <span class="math inline">n \to
\infty</span>.</p>
<p>People often write this in differential form: <span
class="math display">
    dX_t = R_tdt + A_tdB_t.
</span></p>
<p>Suppose that <span class="math inline">f(t, x)</span> is a function
from <span class="math inline">[0, \infty) \times \to \mathbb{R}</span>
that is <span class="math inline">C^1</span> in <span
class="math inline">t</span> and <span class="math inline">C^2</span> in
<span class="math inline">x</span>. Then, <span class="math display">
f(t, B_t) - f(0, B_0) = \int_0^t \partial_s f(s,B_s)ds + \int_0^t
\partial_x f(s, B_s) + \frac{1}{2} \int_0^t\partial^2_{xx}f(s, B_s)ds
</span></p>
<p>Suppose <span class="math inline">f</span> is <span
class="math inline">C^2</span>, but not all of <span
class="math inline">\mathbb{R}</span>, merely on an open interval <span
class="math inline">(a, b)</span>, and <span
class="math inline">B_t</span> is a Brownian motion starting in the
interval. Then, if <span class="math inline">T = \inf \{ t \mid B_t = a,
b\}</span>, then if <span class="math inline">t &lt; T</span>, then
Ito’s formula holds.</p>
<p><em>Def</em>: Suppose that <span class="math inline">dX_t = R_tdt +
A_tdB_t</span>; that is, <span class="math display">
    X_t = X_0 + \int_0^t R_sds + \int_0^t A_sdB_s.
</span> Then, we define <span class="math display">
    \int_0^t Y_sdX_s = \int_0^t Y_sR_sds + \int_0^t Y_sA_sdB_s
</span> and <span class="math display">
    \left\langle X \right\rangle_t = \lim_{n \to \infty} \sum_{j &lt;
t/n} (X_{j/n} - X_{(j-1)/n})^2.
</span></p>
<p>If you look closely at <span class="math inline">X</span>, you can
see that the quadratic variation becomes <span class="math display">
    \left\langle X \right\rangle_t = \lim_{n \to \infty} (A_{j/n} -
A_{(j-1)/n})^2 = \int_0^t A_s^2ds.
</span></p>
<p><strong>Prop</strong>: If <span class="math inline">f(t,x)</span> is
<span class="math inline">C^1</span> in <span
class="math inline">t</span> and <span class="math inline">C^2</span> in
<span class="math inline">x</span>, and <span
class="math inline">X_t</span> is as above, <span class="math display">
    f(t, X_t) - f(0, X_0) = \int_0^t \partial_t f(s,X_s)ds + \int_0^t
\partial_x f(s, X_s)dX_s + \frac{1}{2} \int_0^t \partial_{xx}f(s,
X_s)A^2_sds.
</span></p>
<h3 id="diffusions">Diffusions</h3>
<p><em>Def</em>: <strong>Diffusions</strong> are SDEs of the form <span
class="math display">
    dX_t = m(t, X_t)dt + \sigma(t, X_t)dB_t
</span> where <span class="math inline">m, \sigma</span> are
deterministic. Something satisfies the above if <span
class="math display">
    X_t = y_0 + \int_0^t m(s, X_s)ds + \int_0^t \sigma(s, X_s)dB_s.
</span> for some initial condition <span
class="math inline">y_0</span>.</p>
<p><strong>Prop</strong>: When <span class="math inline">m,
\sigma</span> are uniformly Lipschitz in the latter arguments, a
solution to the above exists.</p>
<p><em>Proof</em>: Let <span class="math inline">m, \sigma</span> be
uniformly Lipschitz with constant <span
class="math inline">\beta</span>. For ease, assume that there is no
<span class="math inline">s</span>-dependence (you don’t need this, but
writing it is annoying). We proceed by Picard iteration.</p>
<p>Let <span class="math inline">X_t^{(0)} = y_0</span>. Define <span
class="math inline">X_t^{(n)}</span> by <span class="math display">
    X_t^{(n)} = y_0 + \int_{0}^t m(X_t^{(n-1)})ds + \int_0^t
\sigma(X_s^{(n-1)})dB_s.
</span></p>
<p>We wish to show <span class="math inline">X_t = \lim_{n \to \infty}
X_t^{(n)}</span> exists (in <span class="math inline">L^2</span>). Look
at <span class="math display">
\begin{align*}
    E[|X_t^{(k+1)} - X_t^{(k)}|^2] &amp;= E \left[ \left| \int_0^t
(m(X_s^{(k)}) - m(X_s^{(k-1)}))ds + \int_0^t (\sigma(X_s^{(k)}) -
\sigma(X_s^{(k-1)}))dB_s \right|^2 \right] \\
    &amp;\leq 2E \left[ \left| \int_0^t (m(X_s^{(k)}) -
m(X_s^{(k-1)}))ds \right|^2 \right] + E\left[ \left| \int_0^t
(\sigma(X_s^{(k)}) - \sigma(X_s^{(k-1)}))dB_s \right|^2 \right] \\
    &amp;\leq E \left[ \left(\int_0^t \beta^2 |X_s^{(k)} -
X_s^{(k-1)}|\right)^2 \right] + \int_0^t E[\sigma(X_s^{(k)}) -
\sigma(X_s^{(k-1)})]^2dB_s \\
    &amp;\leq \beta^2 E[t \int_0^t |X_s^{(k)} - X_s^{(k-1)}|^2ds] +
\beta^2 \int_0^t E[|X_s^{(k)} - X_s^{(k-1)}|^2]ds \\
    &amp;\leq Ct \int_0^t E[|X_s^{(k)} - X_s^{(k-1)}|^2]
\end{align*}
</span> and specifically, one can now show <span class="math display">
    E[|X_t^{k+1} - X_t^{(k)}|] \leq \frac{\lambda^{k+1}t^{k+1}}{(k+1)!}
</span> which vanishes as <span class="math inline">k \to
\infty</span>.</p>
<p><em>Def</em> Let <span class="math inline">X_t</span> be a diffusion
as above. The <strong>generator</strong> of <span
class="math inline">X_t</span> is <span class="math display">
    Lf(x) = \lim_{t \to 0} \frac{E^x[f(X_t)] - E^x[f(X_0)]}{t}.
</span> Let <span class="math inline">f</span> be <span
class="math inline">C^2</span>; then <span class="math display">
\begin{align*}
    f(X_t) - f(X_0) &amp;= \int_0^t f&#39;(X_s)dX_s + \frac{1}{2}
\int_0^t f&#39;&#39;(X_s)d \left\langle X \right\rangle_s \\
    &amp;= \int_0^t f&#39;(X_s)\sigma(X_s)dB_s + \int_0^t
(f&#39;(X_s)m(X_s) + \frac{1}{2}f&#39;&#39;(X_s)\sigma^2(X_s))ds.
\end{align*}
</span> Taking the expectation, under suitable regularity conditions (we
will take <span class="math inline">\sigma, m</span> bounded), we get
that <span class="math display">
    E[f(X_t)] - E[f(X_0)] = 0 + E[t(f&#39;(X_0)m(X_0) +
\frac{1}{2}f&#39;&#39;(X_0)\sigma^2(X_0) + o(t^2)]
</span> and <span class="math display">
    Lf(x) = m(x)f&#39;(x) + \frac{\sigma^2(x)}{2} f&#39;&#39;(x).
</span></p>
<p><strong>Prop</strong>: Let <span class="math display">
    dX_t = R_tdt + A_tdB_s
</span> and <span class="math display">
    dY_t = S_tdt + C_tdB_s.
</span> Then if we define the covariation as <span class="math display">
    \left\langle X, Y \right\rangle_t = \lim_{n \to \infty} \sum_{j &lt;
tn}(X_{j/n} - X_{(j-1)/n})(Y_{j/n} - Y_{(j-1)/n}) = \int_0^t A_sC_sds
</span> we have <span class="math display">
    dX_tY_t = X_tdY_t + Y_tdX_t + d\left\langle X, Y\right\rangle_t.
</span></p>
<p>Let <span class="math inline">B_t = (B_t^1, \dots, B_t^d)</span> be a
Brownian motion. Then, <span class="math display">
    \left\langle B^j, B^k \right\rangle_t = 0
</span> for <span class="math inline">k \neq j</span>. You can then
write multidimensional stochastic integrals as stochastic integrals in
each of the different dimensions; that is, <span class="math display">
    dX_t^{i} = R_t^{i}dt + \sum_{j=1}^d A_t^{(i,j)} dB_t^j.
</span> The covariation is then <span class="math display">
    \left\langle X^{(j)}, X^{(k)} \right\rangle_t = \sum_{i=1}^d A^{(j,
i)}_tA^{(k, i)}_t.
</span></p>
<p><strong>Theorem (Multivariate Itö)</strong>: Suppose <span
class="math inline">f(t, x)</span> is a map from <span
class="math inline">\mathbb{R}^{n + 1} \to \mathbb{R}</span>, and is
<span class="math inline">C^1</span> in <span
class="math inline">t</span> and <span class="math inline">C^2</span> in
<span class="math inline">x</span>. Then, <span class="math display">
    X_t = (X_1^{(1)}, \dots, X_t^{(n)})
</span> satisfies <span class="math display">
    df(t, X_t) = \partial_t f(t, X_t)dt + \sum_{j=1}^{n}
\partial_{x_j}f(t, X_t)dX_t^{(j)} + \frac{1}{2}\sum_{j=1}^n\sum_{k=1}^n
\partial_{x_j x_k}f(t, X_t)d \left\langle X^{(j)}, X^{(k)}
\right\rangle_t.
</span></p>
<p>If <span class="math inline">Z_t = \int_0^t A_sdB_s</span>, then
<span class="math inline">Z_t</span> is not necessarily a martingale,
but it is a local martingale.</p>
<p><em>Def</em>: A process <span class="math inline">Z_t</span> is a
<strong>continuous local martingale</strong> on <span
class="math inline">[0, \tau)</span> (where <span
class="math inline">\tau</span> could be <span
class="math inline">\infty</span>) if there is a sequence of stopping
times <span class="math inline">\tau_1 \leq \tau_2 \leq \cdots</span>
such that a.s. <span class="math inline">\lim_{n \to \infty} \tau_n =
\tau</span> and for each <span class="math inline">n</span>, <span
class="math inline">Z_{t \land \tau_n}</span> is a continuous
martingale.</p>
<p><strong>Prop</strong>: Stochastic integrals are local
martingales.</p>
<p><em>Proof</em>: Take hitting times as stopping times.</p>
<h4 id="feynman-kac">Feynman-Kac</h4>
<p>Let <span class="math inline">X_t</span> be a geometric Brownian
motion <span class="math display">
    dX_t = mX_tdt + \sigma X_t dB_t.
</span></p>
<p>Suppose we have a European option, such that at some fixed time <span
class="math inline">T &gt; 0</span> and fixed price <span
class="math inline">S</span>, we can exercise the option to buy some
asset at <span class="math inline">T</span> for <span
class="math inline">S</span>. Then the value of an option at time <span
class="math inline">T</span> is <span class="math inline">F(X_t)</span>,
where <span class="math display">
    F(x) = (x - s)_+ = \max \{x - s, 0\}.
</span></p>
<p>Let <span class="math inline">\varphi(t, x)</span> be the value of
this option at time <span class="math inline">t &lt; T</span>; that is,
<span class="math display">
    \varphi(t, x) = E[e^{-r(T - t)}F(X_T) \mid X_t = x].
</span> What PDE does <span class="math inline">\varphi(t, x)</span>
satisfy?</p>
<p>More generally, let <span class="math display">
\begin{gather*}
    dX_t = m(t, X_t)dt + \sigma(t, X_t)dB_t \\
    dR_t = r(t, X_t)R_tdt \\
    R_t = R_0 \exp\left( \int_0^t r(s, X_s)ds \right) \\
    \varphi(t, x) = E \left[ \exp\left(-\int_{t}^Tr(s, X_s)ds\right)
F(X_T) \mid X_t = x \right]
\end{gather*}
</span> Now define <span class="math display">
    M_t = E[R_T^{-1}F(X_T) \mid \mathcal F_t] = R_t^{-1}E \left[ \exp
\left(-\int_t^T r(s,X_s)ds\right) F(X_T) \mid \mathcal F_t\right]
</span> so <span class="math display">
    M_t = R_t^{-1}\varphi(t, X_t).
</span> is a martingale.</p>
<p>Assume for now that <span class="math inline">\varphi</span> is
sufficiently regular to apply Ito; then <span class="math display">
\begin{align*}
    d\varphi(t, X_t) &amp;= \partial_t \varphi dt + \partial_x \varphi
dX_t + \frac{1}{2}\partial_{xx} \varphi d \left\langle X \right\rangle_t
\\
    &amp;= \partial_t \varphi dt + \partial_x \varphi(mdt + \sigma dB_t)
+ \frac{1}{2}\partial_{xx} \varphi\sigma^2 dt
\end{align*}
</span> and if you compute you get the following.</p>
<p><strong>Theorem</strong>: Let <span class="math inline">X_t</span> be
a geometric Brownian motion as above. Then, if <span
class="math inline">\varphi</span> is as above and is <span
class="math inline">C^1</span> in <span class="math inline">t</span> and
<span class="math inline">C^2</span> in <span
class="math inline">x</span>, then <span
class="math inline">\varphi</span> satisfies the PDE <span
class="math display">
    \partial_t \varphi(t, x) = -m(t,x) \partial_x \varphi(t,x) -
\frac{1}{2}\sigma(t,x)^2 \partial_{xx} \varphi(t,x) + r(t,x)
\varphi(t,x)
</span> with terminal condition <span class="math inline">\varphi(T,x) =
F(x)</span>.</p>
<p>Suppose that we have some SDE <span class="math display">
    dX_t = m(X_t)dt + \sigma(X_t)dB_t
</span> with <span class="math inline">m, \sigma</span> Lipschitz. Then
the generator is <span class="math display">
    Lf(x) = m(x)f&#39;(x) + \frac{\sigma^2(x)}{2}f&#39;&#39;(x)
</span> and, if we have initial condition <span
class="math inline">F(x)</span>, then <span class="math display">
    u(t, x) = E^x[F(X_t)]
</span> and <span class="math display">
    \partial_t u(t,x) = L_x u(t,x), \ \ u(0, x) = F(x).
</span> On the other hand, if we have a terminal condition at <span
class="math inline">t = T</span>, we have <span class="math display">
    \partial_t\varphi(t, x) = L_x(x, t), \ \ \varphi(T, x) = F(X)
</span> and <span class="math display">
    \varphi(t, x) = u(T - t, x), \ \  \partial_t \varphi(t, x) = -L_x
\varphi(t, x).
</span></p>
<p><em>Example</em>: Suppose we have a geometric Brownian motion with
<span class="math inline">m(x) = mx, \sigma(x) = \sigma x</span>,
alongside some interest rate <span class="math inline">r</span>. Then,
<span class="math display">
    \varphi(t, x) = E[e^{-r(T - t)}F(X_T)]
</span> and <span class="math display">
    \partial_t \varphi(t, x) = r\varphi(t, x) - mx\varphi&#39;(t,x) -
\frac{\sigma^2}{2}x^2 \varphi&#39;&#39;(t, x).
</span></p>
<h3 id="integrals-against-continuous-martingales">Integrals Against
Continuous Martingales</h3>
<p><em>Def</em>: If we have <span class="math inline">f:[0, 1] \to
\mathbb{R}</span>, its <strong>variation</strong> is <span
class="math display">
    V_f = \sup_{0 = t_0 &lt; \dots &lt; t_n = 1} \sum_{j=1}^n |f(t_j) -
f(t_{j-1})|.
</span> We say that <span class="math inline">f</span> has
<strong>bounded variation</strong> if <span class="math inline">V_f &lt;
\infty</span>.</p>
<p><strong>Prop</strong>: Let <span class="math inline">M_t</span> be a
continuous martingale with respect to the filtration <span
class="math inline">\{ \mathcal F_t \}</span> and <span
class="math inline">M_0  = 0</span>. If <span
class="math inline">M_t</span> has paths of bounded variation, then
<span class="math inline">M_t = 0</span> for all <span
class="math inline">t</span> with probability 1.</p>
<p><em>Proof</em>: We will show that <span class="math inline">E[M_1^2]
= 0</span>. Then, in the case <span class="math inline">V_M(1) \leq K
&lt; \infty</span>, <span class="math display">
    E[M_1^2] = E \left[ \sum_{j=1}^n (M_{j/n} - M_{(j-1)/n})^2 \right].
</span> Bound the inner sum by <span class="math inline">\delta_n
\sum_{j=1}^n |M_{j/n} - M_{(j-1)/n}| \leq \delta_n K</span> where <span
class="math inline">\delta_n</span> is the maximal increment. Since
<span class="math inline">M_t</span> is continuous, <span
class="math inline">\delta_n K \to 0</span>. If we do not have that
uniform bound, set <span class="math inline">\tau_K = \min \{ t:  V_M(t)
= K \}</span>. Then we have <span class="math inline">E[M_{1 \land
\tau_K}^2] = 0</span> and we conclude by monotone convergence.</p>
<p><em>Def</em>: The quadratic variation <span
class="math inline">\left\langle M \right\rangle_t</span> is the unique
increasing process such that <span class="math inline">M_t^2 -
\left\langle M \right\rangle_t</span> is a martingale.</p>
<p><strong>Theorem</strong>: If <span class="math inline">M_t</span> is
a continuous martingale with respect to <span
class="math inline">\mathcal F_t</span> with quadratic variation <span
class="math inline">\left\langle M \right\rangle_t</span>, then <span
class="math inline">M_t</span> is a standard Brownian motion ith respect
to <span class="math inline">\mathcal F_t</span>.</p>
<p><em>Proof</em>: Certainly <span class="math inline">M_0 = 0</span>
and we have continuous paths. We only need to show that <span
class="math inline">E[e^{i\lambda (M_t - M_s)} \mid \mathcal F_s] =
e^{-\frac{\lambda^2(t-s)}{2}}</span>. Since the definitions of the Ito
integral and Ito’s formula go through with only the quadratic variation
assumption, we apply the Ito formula to <span class="math inline">f(x) =
e^{i\lambda x}</span>, so <span class="math display">
    f(M_t) - f(M_0) = \int_0^t f&#39;(M_s)dB_s + \frac{1}{2} \int_0^t
f&#39;&#39;(M_s)ds
</span> and in expectation <span class="math display">
    E[f(M_t) - f(M_0)] = -\frac{\lambda^2}{2} \int_0^t E[f(M_s)]ds
</span> since <span class="math inline">f&#39;&#39; =
-\lambda^2f</span>. Then just solve the differential equation.</p>
<p>Take a standard Brownian motion <span class="math inline">B_s, 0 \leq
s \leq 1</span>; the Wiener measure <span class="math inline">\mathcal
W</span> is a measure on the Borel <span
class="math inline">\sigma</span>-algebra of <span
class="math inline">C[0,1]</span>, the space of continuous functions
with the <span class="math inline">\sup</span> norm. Let <span
class="math inline">B(f, \epsilon) = \{ g \mid |f - g| &lt;
\epsilon\}</span>. Then, <span class="math display">
    \mathcal W[B(f, \epsilon)] = P(|f(t) - B_t| &lt; \epsilon).
</span> Put <span class="math inline">\mathcal W_{m, \sigma^2}</span> if
we have a drift/variance term.</p>
<p><strong>Prop</strong>: <span class="math inline">\mathcal W_{0, 1}
\perp \mathcal W_{0, \sigma^2}</span> for <span
class="math inline">\sigma^2 \neq 1</span>. On the other hand, <span
class="math inline">\mathcal W_{0, 1}</span> is mutually absolutely
continuous with <span class="math inline">\mathcal W_{m, 1}</span>.</p>
<p><em>Proof</em>: The first part is easy. Look at <span
class="math inline">A</span>, the functions with quadratic variation 1;
then <span class="math inline">\mathcal W_{0, 1}(A) = 1</span>, <span
class="math inline">\mathcal W_{0, \sigma^2}(A) = 0</span>. In the
second case, we go to a weak version of the Girsanov theorem.</p>
<p><strong>Theorem</strong>: <span class="math inline">\mathcal W</span>
and <span class="math inline">\mathcal W_{m, 1}</span> are mutually
absolutely continuous and <span class="math display">
    \frac{d\mathcal W_{m,1}}{d \mathcal W} = \exp\left(mB_1(\omega) -
\frac{1}{2}m^2\right).
</span></p>
<p><em>Proof</em>: Let <span class="math inline">B_t</span> be a
standard Brownian motion, and define <span class="math inline">M_t =
\exp(mB_t - \frac{m^2t}{2})</span> so that <span
class="math inline">dM_t = mM_t dB_t</span>. On <span
class="math inline">\mathcal F_t</span>, define a probability measure
<span class="math inline">Q_t</span> such that <span
class="math display">
    Q_t[V] = E[1_V M_t]
</span> for all <span class="math inline">V \in \mathcal F_t</span>; by
conditioning on <span class="math inline">\mathcal F_s</span>, we can
see that if <span class="math inline">s &lt; t</span> and <span
class="math inline">V \in \mathcal F_s</span>, then <span
class="math inline">Q_s(V) = Q_t(V)</span>. It is clear that the
Radon-Nikodym derivatives are just given by <span
class="math inline">M_t</span> and <span
class="math inline">1/M_t</span>.</p>
<p>Now define <span class="math inline">Q</span> a probability measure
on <span class="math inline">\mathcal F_\infty</span> such that if <span
class="math inline">\mathcal F_{t}</span>, <span
class="math inline">Q(V) = E[1_V M_t]</span>, which is well defined
because of the above. Moreover, if <span class="math inline">X</span> is
<span class="math inline">\mathcal F_t</span> measurable, this gives
that <span class="math display">
    E_Q[X] = E[X M_t].
</span> Now we claim that <span class="math inline">B_t</span> is a
Brownian motion under <span class="math inline">Q</span> with variance
parameter 1 and drift <span class="math inline">m</span>. Certainly it
holds that <span class="math inline">B_0 = 0</span> and <span
class="math inline">t \mapsto B_t</span> is continuous with probability
1 under <span class="math inline">Q</span> and <span
class="math inline">P</span> (since they are mutually absolutely
continuous), and if <span class="math inline">s, t &gt; 0</span> then
the conditional distribution of <span class="math inline">B_{t + s} -
B_s</span> given <span class="math inline">\mathcal F_s</span> is <span
class="math inline">N(mt, t)</span>. In fact, all we need to show then
is that <span class="math display">
    E_Q[\exp(\lambda(B_{t+s} - B_s)) \mid \mathcal F_s] =
\exp\left(\lambda mt + \frac{\lambda^2 m^2 t }{2}\right).
</span> This essentially boils down to the definition of the conditional
expectation: write it down and unwrap in terms of <span
class="math inline">P</span>-expectations and win.</p>
<p>In the above omitted computation, we essentially do a simple version
of the following theorem.</p>
<p><strong>Theorem</strong>: Let <span class="math inline">B_t</span> be
a Brownian motion on a probability space <span
class="math inline">(\Omega, \mathcal F, P)</span>. Suppose we have a
nonnegative martingale <span class="math inline">M_t, M_0 = 1</span>
satisfying <span class="math display">
    dM_t = A_tM_t dB_t.
</span> Then, <span class="math inline">M_t = M_0e^{Y_t}</span> where
<span class="math display">
    Y_t = \int_0^t A_sdB_s - \frac{1}{2} \int_0^t A_s^2 ds.
</span> This follows by a simple application of Ito. Define a measure
<span class="math inline">Q_t</span> on <span
class="math inline">\mathcal F_t</span>, given by <span
class="math display">
    Q_t[V] = E[M_t 1_V]
</span> so that <span class="math inline">\frac{dQ_t}{dP} = M_t</span>
and if <span class="math inline">s &lt; t</span> and <span
class="math inline">V \in \mathcal F_s</span>, <span
class="math inline">Q_s[V] = Q_t[V]</span>. Then, we define <span
class="math inline">Q</span> as before. Let <span
class="math inline">X_t = B_t - \int_0^t A_sds</span>; then <span
class="math inline">X_t</span> is a standard Brownian motion on <span
class="math inline">(\Omega, \mathcal F, Q)</span>.</p>
<p><em>Proof</em>: We just do a heuristic argument. As an approximation,
<span class="math display">
    B_{t + \Delta t} - B_t = \begin{cases}
        \sqrt{\Delta t} &amp; \text{probability } 1/2 \\
        -\sqrt{\Delta t} &amp; \text{probability } 1/2 \\
    \end{cases}
</span> and similarly <span class="math inline">M_{t + \Delta t} = M_t(1
\pm A_t \sqrt{\Delta t})</span> equiprobably as well. In the new
measure, this is essentially tilting the probabilities by <span
class="math inline">1 \pm A_t \sqrt{\Delta t}</span> and so <span
class="math inline">E_Q[B_{t + \Delta t} - B_t] = A_t \Delta
t</span>.</p>
<p>Check that <span class="math inline">X_t</span> is a <span
class="math inline">Q</span>-martingale and conclude by noting that it
has quadratic variation <span class="math inline">t</span>.</p>
<h3 id="conformal-invariance">Conformal Invariance</h3>
<p>Let <span class="math inline">B_t = (B_t^1, B_t^2)</span> be a two
dimensional Brownian motion; identify it to <span
class="math inline">B_t^1 + iB_t^2</span>.</p>
<p>Let <span class="math inline">f(B_t) = u(B_t) + iv(B_t)</span>. Then
<span class="math display">
    du(B_t) = \nabla u \cdot dB_t + \frac{1}{2} (\Delta u) dt
</span> and <span class="math display">
    dv(B_t) = \nabla v \cdot dB_t + \frac{1}{2} (\Delta v) dt
</span> but if <span class="math inline">f</span> is holomorphic, then
the Laplacians vanish and we use the Cauchy-Riemann equations to
simplify to <span class="math display">
    du(B_t) = u_x(B_t)dB_t^{1} + u_y(B_t)dB_t^2
</span> and <span class="math display">
    dv(B_t) = v_x(B_t)dB_t^{1} + v_y(B_t)dB_t^2
</span> which have quadratic variations <span class="math display">
   d\langle u(B_t) \rangle_t = d\langle v(B_t) \rangle = (u_x^2 +
u_y^2)dt = |f&#39;(B_t)|^2  dt.
</span></p>
<p>Define <span class="math inline">T(t) = \int_0^T |f&#39;(B_s)|^2ds =
T</span>. Then <span class="math inline">Y_t = f(B_{T(t)})</span> is a
standard complex Brownian motion.</p>
<h3 id="levy-processes">Levy Processes</h3>
<p><em>Def</em>: A <em>Levy process</em> is a stochastic process that
satisfies</p>
<ul class="incremental">
<li><span class="math inline">X_0 = 0</span> almost surely;</li>
<li>stationary increments: <span class="math inline">X_t - X_s \sim X_{t
- s}</span>;</li>
<li>independent increments;</li>
<li>continuity in probability: <span class="math inline">X_t \to
0</span> in probability as <span class="math inline">t \to
0</span>.</li>
</ul>
<p><em>Def</em>: A <em>compound Poisson process</em> is composed of a
Poisson process <span class="math inline">N_t</span> with some rate
<span class="math inline">\lambda &gt; 0</span>, alongside <span
class="math inline">Y_1, Y_2, \dots</span> mutually independent of
themselves and <span class="math inline">N_t</span> and identically
distributed random variables with distribution <span
class="math inline">\mu^\sharp</span>. Then, <span class="math display">
    X_t = \sum_{i=1}^{N_t} Y_i
</span> is the compound Poisson process.</p>
<p><em>Def</em>: A random variable <span class="math inline">X</span>
has an <em>infinitely divisible</em> distribution if for each <span
class="math inline">n</span> we may find <span class="math inline">Y_1,
\dots Y_n</span> iid such that <span class="math inline">X \sim
\sum_{i=1}^n Y_i</span>.</p>
<p><strong>Prop</strong>: Levy processes are infinitely divisible at any
time.</p>
<p>Now look at the characteristic functions of a Levy process. In
particular define the characteristic exponent <span
class="math inline">\Psi(s)</span> where <span class="math display">
    \varphi_{X_1}(s) = \exp(\Psi(s))
</span> and note that <span class="math display">
    \varphi_{X_t}(s) = (\varphi_{X_1}(s))^t.
</span></p>
<p>Let <span class="math inline">X_t, Y_1, \dots</span> be as in a
compound Poisson process. Then, if we set <span class="math display">
    \varphi(s) = \varphi_{Y_j}(s) = \int_{-\infty}^\infty
\exp(isx)\mu^\sharp (dx)
</span> we have <span class="math display">
    \varphi_{X_1}(s) = \sum_{n=0}^\infty P(N_1 = n) E[e^{sX_1} \mid N_1
= n] = \sum_{n=0}^\infty \frac{\lambda^n}{n!} e^{-\lambda}
(\varphi(s))^n = \exp(\lambda(\varphi(s) - 1))
</span> or equivalently, <span class="math display">
    \Psi(s) = \int_{-\infty}^\infty (e^{isx} - 1)\mu(dx)
</span> where <span class="math inline">\mu = \lambda \mu^\sharp</span>.
We call <span class="math inline">\mu</span> the Levy measure for <span
class="math inline">X_t</span>.</p>
<p><strong>Prop</strong>: There are a few properties that we can say: as
<span class="math inline">t \to 0</span>,</p>
<ul class="incremental">
<li><span class="math inline">P(N_t = 0) = 1 - \lambda t +
o(t)</span>;</li>
<li><span class="math inline">P(N_t \geq 2) = o(t)</span>;</li>
<li><span class="math inline">P(X_t \in (a, b)) = \mu(a,b)t +
o(t)</span> for <span class="math inline">0 \notin (a, b)</span>.</li>
</ul>
<p><em>Def</em>: We define the generator of a Levy process as <span
class="math display">
    Lf(x) = \lim_{t \to 0}\frac{E^x[f(X_t) - f(x)]}{t}
=  \int_{-\infty}^\infty (f(x + y) - f(x)) \mu(dy).
</span></p>
<p><strong>Prop</strong>: If <span class="math inline">X_t, Y_t</span>
are independent Levy processes, then <span class="math inline">X_t +
Y_t</span> is a Levy process with characteristic exponent <span
class="math inline">\Psi_X + \Psi_Y</span> and generator <span
class="math inline">L_x + L_y</span>.</p>
<p><em>Def</em>: Let <span class="math inline">X_t</span> be a Levy
process with measure <span class="math inline">\mu</span>, and put <span
class="math display">
    m = \int_{-\infty}^\infty x\mu(dx), \ \sigma^2 =
\int_{-\infty}^\infty x^2 \mu(dx).
</span> Then <span class="math inline">E[X_t] = mt</span> and <span
class="math inline">\mathop{\mathrm{Var}}(X_t) = \sigma^2 t</span>.</p>
<p><em>Def</em>: A function <span class="math inline">f:[0, \infty) \to
\mathbb{R}</span> is called cadlad (continue a droite, limite a gauche)
if the paths are right-continuous and for all <span
class="math inline">t</span>, <span class="math inline">f(t-)</span>
exists.</p>
<p><em>Def</em>: If <span class="math inline">X_t</span> is compound
Poisson, then <span class="math display">
    \int_0^t A_s dX_s = \sum A_s (X_s - X_{s-})
</span> where the sum is over jump times.</p>
<p>The issue is that if <span class="math inline">A_t</span> is adapted,
then <span class="math inline">A_t = X_t</span> shows that <span
class="math inline">\int A_s dX_s = 1</span> at the first jump, and is
not a martingale; in fact we need <span class="math inline">A_t</span>
adapted to <span class="math inline">\mathcal F_{t-}</span> instead.</p>
</body>
</html>
